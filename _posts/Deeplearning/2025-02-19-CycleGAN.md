---
layout: single
title: "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(CycleGAN)"

excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-02-19
last_modified_at: 2025-02-19
classes: #wide    
---
[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(CycleGAN)](https://arxiv.org/pdf/1703.10593){: .btn .btn--primary}


# Abstract
![Image5](/assets/images/cycleGAN/image1.jpg){: .align-center}
&nbsp;&nbsp; Image-to-Image translation은 image pair를 이용하여 input image와 output image 사이의 mapping을 학습하는 것이 목표이다. 그러나 많은 task에서 paired image를 얻기는 어렵다. CycleGAN은 이러한 한계를 극복하기 위해 Source Domain $X$로 부터 Target Domain $Y$로 mapping을 학습시켰고, 생성한 이미지 $G(X)$가 Y와 구분되지 않도록 adversarial loss를 사용하였다. 그러나 이러한 방법은 제약이 부족하기 때문에 한가지 mapping을 추가하였다. 이것은 $ F : Y $\to$ X$이고 Loss로 $F(G(X)) = X$ 가 되도록 Cycle Consistent를 도입하였다. 

# 1. Introduction

![Image5](/assets/images/cycleGAN/image2.jpg){: .align-center}

&nbsp;&nbsp; cycleGAN은 위 그림처럼 style의 차이를 파악할 수 있고, style을 translation한다면 어떻게 보일지 상상할 수 있다. 이 논문에서는 한 이미지의 special characteristics을 파악할 수 있고, 이것을 paired image 없이 다른 이미지에 translation 시킬 수 있다.


&nbsp;&nbsp; 이러한 문제는 image-to-image translation으로 더욱 광범위하게 나타난다. 예를 들어 주어진 $x$로 부터 또다른 이미지 $y$로 변환하는 grayscale to color, image to semantic label, edge-map to photograph 등이 있다. 이전 연구들에서는 paired data를 가지고 좋은 성능을 보이는 모델들을 많이 보여왔다. 그러나 현실에서 paired data를 수집하는 것은 아주 어려운 일이다. 따라서 논문에서는 paired data 사용 없이 source domain에서 target domain으로 image를 변환하는 알고리즘을 찾아내었다. 저자들은 $G: X \to Y$ 의 mapping을 학습시키고 이미지 $y$와 구분할 수 없도록 $x$로 부터 $\hat{y}$를 생성한다. 이론적으로 이 목적은 output $\hat{y}$가 empirical distribution $p_{data}(y)$로 부터 유도될 수 있도록한다. 
&nbsp;&nbsp; 그러나 이러한 translation이 $x$ $\to$ $y$를 보장하지 않는다. 여기에는 무수히 많은 $G$가 존재하고 adversarial objective를 최적화 하는 것이 어렵다. 표준 절차는 종종 모든 input image가 동일한 output image로 맵핑되고 최적화가 진행되지 않는 문제를 초래하기도 한다. 그러므로 이러한 문제를 해결하기 위해 논문에서는 **더 많은 구조**를 추가한다. 이것은 **Cycle Consistent**라는 것으로 English to French에서 French to English로 돌아오는 것과 유사한 성질이다. 수학적으로 만약 $ G: X \to Y$와 $F: Y \to X$의 mapping을 가지고 있다면, 이것은 서로 reverse이고, 서로 사영이다. 논문에서는 이 $G와 F$를 동시에 학습 시키고, <span style='color:red'>Cycle Consistent loss </span> 인 $F(G(x)) \approx x$, $G(F(y)) \approx y$를 추가하여 adversarial loss와 결합시킨다.

![Image5](/assets/images/cycleGAN/image3.jpg){: .align-center}

# 2. Related work

* Generative Adversarial Networks(GANs)

* Image-to-Image Translation

* Unpaired Image-to-Image Translation

* Cycle Consistency

* Neural Style Transfer

# 3. Formulation

&nbsp;&nbsp; CycleGAN의 목적은 주어진 두 domain $X, Y$의 mapping function을 학습시키는 것이다. 

* $\{x_i\}_{i=1}^{N}, x_i \in X, (x - p_{data}(x))$

* $\{y_j\}_{j=1}^{N}, y_j \in Y, (y - p_{data}(y))$

&nbsp;&nbsp; 이 모델은 두개의 mapping $G : X \to Y, F : Y \to X$를 포함하고 있고, 두 개의 discriminator $D_X, D_Y$를 포함하고 있다. 여기서 $D_X$는 $ {x}와 F(y)$를 구분할 수 없도록 도와주고, 마찬가지로 $D_Y$는 ${y}와 F(X)$를 구분할 수 없도록 한다. 우리의 목적은 adversarial loss로 생성된 이미지의 분포와 실제 분포를 매칭시키고, cycle consistency loss로 서로 모순되는 G, F의 mapping을 예방하는 것이다.

## 3.1 Adversarial Loss

&nbsp;&nbsp; 논문에서는 adversarial loss를 두가지 mapping function G, F에 적용한다. mapping G에 대해서 Loss는 다음과 같다. F에 대해서도 비슷하게 Loss가 구성된다.

![Image5](/assets/images/cycleGAN/image4.jpg){: .align-center}

* $G$ : Generator
* $D$ : Discriminator

## 3.2 Cycle Consistency Loss

&nbsp;&nbsp; Adversarial Loss는 이론상 G, F를 학습하여 다른 분포의 output을 뽑아낼 수 있다. 그러나 무수히 많은 경우의 수가 존재하고, 여러가지 input을 넣더라도 같은 output이 나오는 경우가 발생할 수 있다. 따라서 adversarial loss는 $x_i$로 부터 $y_i$를 출력하는 것을 보장하지 않는다. 가능한 mapping function의 공간을 감소 시키기 위해, 논문에서는 cycle-consistency가 필요하다고 주장한다. cycle-consistency는 original image가 $G와 F$를 통과했을 때 다시 original image와 유사한 결과가 나와야 한다는 개념이다. 수식적으로 표현하면 다음과 같다.

$$ x \to G(x) \to F(G(x)) \approx x$$
$$ y \to G(y) \to F(G(y)) \approx y$$

논문에서는 Cycle Consistency Loss를 다음과 같이 정의한다.
![Image5](/assets/images/cycleGAN/image5.jpg){: .align-center}

&nbsp;&nbsp; 또한 예비 실험에서 $L1$ norm 대신 $F(G(x))와 x, G(F(y))와 y$의 adversarial loss를 대신 사용해 보았는데, 큰 성능 향상은 나타나지 않았다. 
![Image5](/assets/images/cycleGAN/image6.jpg){: .align-center}

## 3.3 Full Objective

$$
\mathcal{L}(G, F, D_X, D_Y) =
\mathcal{L}_{GAN}(G, D_Y, X, Y) +
\mathcal{L}_{GAN}(F, D_X, Y, X) +
\lambda \mathcal{L}_{cycle}(G, F),
$$

$$
G^*, F^* = \arg\min_{G, F} \max_{D_X, D_Y} \mathcal{L}(G, F, D_X, D_Y).
$$

&nbsp;&nbsp; 주목할 점은 이 모델이 2개의 autoencoder로 학습하는 관점으로 볼 수 있다는 것이다. 
$$F \circ G : X \to X, G \circ F : Y \to Y$$
&nbsp;&nbsp; 이 autoencoder는 특별한 내부 구조를 가진다. 이미지를 또 다른 domain으로 변환 시킬때 intermediate representation을 통해 이미지를 자기 자신에 맵핑한다. 

# 4. Implementation

## 4.1 Network Architecture

&nbsp;&nbsp;논문의 architecture로 generative network 모델은 style transfer, super-resolution 성능이 향상된 Perceptual losses for real-time style transfer and super-resolution의 논문에서 나온 모델을 채택하였다. 이 모델은 세 가지 convolution이 포함되어 있다.

    1. several residual blocks
    2. two fractionally-strided convolutions with stride 1/2
    3. one convolution that maps features to RGB

&nbsp;&nbsp; 또한 128X128 image에 대해서는 6개의 block, 256X256 image에 대해서는 9개의 block을 사용하였다. instance normalization을 사용했고, Discriminator 70X70 PatchGANs을 사용하였다.  

## 4.2 Training details

1. $\mathcal{L}_{GAN}$대신 least-square를 사용하여 negative log likelihood를 사용함
2. 모델의 진동(?)을 줄이기 위해 생성된 이미지의 이력을 사용하여 discriminator를 update
3. Adam solver, batch size : 1, learning rate : 0.0002
4. $\lambda = 10$ 

# 5. Results

&nbsp;&nbsp; 이 논문은 input-output pair의 ground truth가 존재하여 평가할 수 있는 dataset으로 최근에 나온 unpaired translation method와 비교한다. 또한 adversarial loss와 cycle consistency의 중요성을 연구하고, 모델의 일반화 성능을 도출할 것이다. evaluation dataset과 metric은 pix2pix와 동일하게 사용하였으며, 몇가지 베이스라인 모델과 질적, 양적으로 비교한다. 
아래 그림을 보면 cycleGAN은 supervised learning인 pix2pix와 유사한 성능을 보이는 것을 확인할 수 있다. 또한 정량 평가에서도 다른 unpaired 방법들과 달리 pix2pix에 가까운 성능을 보이는 것을 확인할 수 있다.

* Metrics : **AMT, FCN Score, IoU(Semantic segmentation)** 

* Baseline : **CoGAN(latent), SimGAN($| x - G(x) \|_1$), Feature loss(VGG-16) + GAN, BiGAN, pix2pix**

![Image5](/assets/images/cycleGAN/image7.jpg){: .align-center}
![Image5](/assets/images/cycleGAN/image8.jpg){: .align-center}
