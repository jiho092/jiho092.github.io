---
layout: single
title: "NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night"


excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-03-18
last_modified_at: 2025-03-18
classes: #wide    
---
[NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night](https://arxiv.org/pdf/2204.05538){: .btn .btn--primary}

# Abstract

&nbsp;&nbsp; 야간 image segmentation은 자율 주행과 같은 분야에서 중요한 문제지만, 상대적으로 주간 segmentation보다 관심을 받지 못했다. 이 논문에서는 NightLab이라는 야간 Image segmentation framework를 제안한다. 다양한 deeplearning모델에 **night aware features**를 적용하여 SOTA를 달성했다.

&nbsp;&nbsp; NightLab은 두개의 level 모델을 포함한다.

1. Image level
2. Region level

&nbsp;&nbsp; 각 level은 light adaptation module과 segmentation module로 구성된다. image level에서는 전체적인 초기 segmentation 결과를 제공하고 동시에 hardness detection module이 분석이 더 필요한 영역을 식별하고 집중하여 성능을 향상시킨다. end-to-end 방식이며 NightCity, BDD100K 등 데이터셋에서 좋은 성능을 보였다.

# 1. Introduction
![Image5](/assets/images/Nightlab/image1.jpg){: .align-center}
&nbsp;&nbsp; Semantic Segmentation은 Deeplab, Transformer 등의 기법이 많이 사용되었지만, 주간 데이터를 많이 다루었다. 야간이미지 연구는 미미한 수준이다. 야간 이미지 분할에서 문제점을 알아보자

## night segmentation의 문제점

1. 야간 이미지에 label이 존재하는 benchmark dataset의 부재
2. 대부분의 야간이미지는 label이 없거나 제한적
3. 이를 해결하기 위해 UDA 방법을 적용
4. 그러나 Domain Gap으로 인한 성능 차이가 큼

## NightCity Dataset

* 고밀도 label image로 구성된 데이터로 UDA보다 좋은 성능을 보여줌

## NightLab

* 논문에서 제안한 Night image segmentation framework

* 야간 조명 변화(illumination variation) 문제 해결

* 특징
![Image5](/assets/images/Nightlab/image2.jpg){: .align-center}

1. dual-level 구조
    * Image-level : 전체 image를 예측
    * Region-level : 어려운 영역을 선택하여 정밀 분석

2. ReLAM(Regularzed light adaptation module) 도입
    * 단순 노출 조정 대신, 조명 정규화를 위해서 light adaptation module 도입
    * 야간 textual를 유지하면서 주,야간 domain gap을 줄임

3. HDM(Hardness Detection Module) 도입
    * 감지하기 어려운 영역을 골라 region model에 전송
    * ex) 도로 위 자전거, 조명 기둥과 같은 애매한 object
4. Swin Transformer encoder + DeformConv decoder 사용

* 모델 동작 방식

1. 이미지 $\to$ ReLAM으로 조명 정규화
2. image level 파싱
3. 동시에 HDM이 어려운 영역 식별
4. 영역을 잘라 region-level model로 보내서 분석
5. image level + region level을 병합하여 최종 예측

# 2. Related work

## 1. Semantic Segmentation

## 2. Domain Adaptation for Segmentation

## 3. Vision tasks in the dark

# 3. NightLab

![Image5](/assets/images/Nightlab/image2.jpg){: .align-center}

위 이미지를 보면 NightLab이 image-level, Region-level로 구성된 것을 알 수 있다.

각 level에는 Segmentation module $\Phi_{Seg}$가 있고, night image generalization 성능을 향상시키기 위한 ReLAM이 있다.

Region level은 어려운 영역만 처리하고, Region Proposal Network $\Phi_{det'}^R$ 에 의해 탐지된다. 이 네트워크는 RDN(Region Detection Network) 혹은 HDM(Hardness Detection Module)이라고 불린다.

최종적으로 두 level의 결과가 병합되어 prediction mask가 출력된다.

## 3.1 Core modules at image and region levels

### ReLAM(Regularized Light Adaptation Module)

* 목적 : 야간 이미지의 light를 조정하여 주간 이미지처럼 보이되 texture 왜곡을 줄이는 방향으로 보정

* 구조

    * Generator $\Phi_{light}$ : style transfer network (ResNet)
    * Input : $I$ RGB 야간 image
    * Output : $L = \Phi_{light}(I)$ RGB light map 
    * 최종 output Image : $I' = I + L$

논문에서는 $\Phi_{light}$ 를 학습시키기 위해 자체적으로 수집한 주간 이미지 $I_d$와 야간이미지 $I_t$ pair를 사용하며, 두가지 Loss를 사용한다.

1. SSIM : image 내부 texture 보존

$$
L_S = \sum_{t \in T_d, t'} \left( 1 - SSIM(I_t, I_t') \right)
$$

2. GAN Loss : 조정된 이미지가 야간 또는 주간인지 식별하기 어렵도록 만듦

$$
L_P(D) = \sum_{t \in T_d, t'} \left( \log D(I_t) + \log(1 - D(I_t')) \right)
$$

최종 Loss는 다음과 같다.
$$
L_{\text{light}} = L_S + L_P(D)
$$

ReLAM은 Image level($\Phi_{light}^I$)와 Region level ($\Phi_{light}^R$)에서 모두 학습된다. 실험에서는 CycleGAN을 사용하여 생성된 이미지를 비교 대상으로 사용하였다. CycleGAN은 texture 왜곡이 존재하는 반면 ReLAM은 정규화를 바탕으로 architecture와 loss에서 더 좋은 segmentation 성능을 보였다.

### Image level Segmentation module

Image level Segmentation module $\Phi_{seg}^I$는 Swin-Transformer와 UperNet을 기반으로 구성된다. 또한 UperNet에 context modeling ability를 향상시키기 위해 Deformable Convolution을 추가한다. 이 네트워크를 논문에서는 **NightLab-Baseline**이라고 부른다.

Input은 enhanced image를 사용하고 output은 해당하는 Ground Truth이다. 또한 Cross-Entropy loss $L_{Seg}^I$를 사용하여 Swin Transformer와 같은 setting으로 학습한다.

### Region-level Segmentation module

night image에서는 lighting, scales와 같은 다양한 차이로 인해 image-level segmentation만으로는 충분하지 않다. 이 때 작고 어두운 조명을 가진 object를 탐지하기 위해서는 zoom in 뷰가 필요하다.

이 문제를 해결하기 위해 Region-level Segmentation module을 도입한다. 이것은 image level에서 탐지가 어려운 영역 $R_{hard}$에 초점을 둔다.

**Region level network $\Phi_{seg}^R$은 image level과 동일한 구조를 사용하지만 class의 수는 다르다.**

* $R_{hard}$ 선택 방법

1. image level prediction $\Phi_{seg}^I$에서 IoU를 계산
2. IoU가 낮은(<0.5) class를 $C_{hard}$로 정의
3. 나머지 class는 $C_{easy}$로 분리

특정 class에 대해 IoU가 낮은 경우, 해당 class를 hard class로 간주하고 이미지 내 instance를 잘라낸다(**bounding box crop**)

이렇게 잘라낸 이미지는 고정된 크기로 확대되며, Region level Segmentation Network $\Phi_{seg}^R$의 학습에 사용된다. 이 데이터과 Cross-entropy를 기반으로 학습된다.

$$
L_{\text{seg}}^R = \sum_{P_j \in \mathcal{P}_R,\; Y_j^R \in \mathcal{Y}_R} CE(P_j,\; Y_j^R)
$$


- $ P_j = \Phi_{\text{seg}}^R(I_j^R) $
  : 잘라낸 이미지 $I_j^R$ 에 대해 영역 수준 세그멘테이션 네트워크의 예측 결과

- $ I_j^R \in \mathcal{I}_R $
  : 잘라낸 이미지 (Region patch)

- $ \mathcal{Y}_R $
  : 해당 잘라낸 이미지들의 GT 레이블 집합

- $ CE(\cdot) $
  : Cross-Entropy Loss

## 3.2 Self-detecting hard regions at night

region-level module은 추론 시 GT 없이 바로 잘라낼 수 없기 때문에, hard regions을 corp하기 위해 직접적인 사용이 불가능하다.

한가지 해결책은 Image level prediction은 $\Phi_{seg}^I$를 활용하여 $R_{hard}$를 생성하는 것이다. 그러나 문제는 $\Phi_{seg}^I$의 예측이 항상 정확하지 않다는 점이다. 이러한 이유로 논문에서는 RDN $\Phi_{det}^R$을 학습시켜 $R_{hard}$에 해당하는 instance를 자동으로 탐지하도록한다.

RDN 구조는 Faster R-CNN에서 영감을 받았으며, RPN(region proposal network)의 구조를 채택한다. hardness region을 탐지하기 위해 먼저 예측된 mask로 부터 pseudo annotation box를 생성하고, 이 박스들을 수동, 자동으로 $\Phi_{seg}^I$의 예측에 기반하여 라벨링한다. 그 이후 RDN은 아래 Loss를 통해 학습된다.

$$L_{det}^R = L_{reg}^R+L_{cls}^R$$

* Bounding box regression loss

$$
L_{\text{reg}}^R = \sum_{r_i \in R_{\text{hard}}} \text{smooth}_{L1}(t(r_i),\; t(\hat{r}_i))
$$

* Classification Loss

$$
L_{\text{cls}}^R = \sum_{r_i \in R_{\text{hard}}} CE(p_{r_i},\; y_{r_i})
$$

- $t(\cdot)$: boinding box를 나타내는 tuple로  
  $(x, y, dw, dh)$ 형식의 회귀값

- $y_{r_i}$: semantic mask에서 얻은 classfication label

- $p_{r_i}$: 예측된 클래스 확률

- 각 이미지당 **최대 10개의 제안 박스**를  
  **non-maximum suppression**을 통해 선택함

![Image5](/assets/images/Nightlab/image4.jpg){: .align-center}

위 그림처럼 RDN은 rider와 motocycle과 같은 겹쳐진 object를 정확히 처리하는 것을 볼 수 있다. 각 proposal box는 image 일부를 포함하고 있어도 해당 객체를 성공적으로 커버할 수 있다. 이러한 방식으로 생성된 $R_{hard}$는 이후 region-level segmentation을 위한 input으로 사용되고, RDN으로부터 생성된 예측 결과 $P^R$과 image-level 예측 $P^I$를 결합하여 $R_{hard}, R_{easy}$를 더 잘 분류한다.

## 3.3 Detecting hard regions with contexts


![Image5](/assets/images/Nightlab/image5.jpg){: .align-center}

RDN의 문제점은 대부분의 proposal region이 Region level Network $\Phi_{seg}^R$이 필요한 context를 포함하지 못한다는 것이다. 위 그림처럼 RDN이 생성한 'rider' proposal 대부분은 rider만 포함하고 오토바이는 포함하지 않는다. 이러한 proposal에 따라 image를 잘라내면 해당 영역은 $\Phi_{seg}^R$에서 person으로 잘못 분류될 수 있다.

이 챕터에서는 RDN보다 개선된 **Hardness Detection Module(HDM)**을 제안한다. HDM은 context를 고려한 영역을 제안하도록 학습되며, $\Phi_{seg}^R$의 예측을 잘 돕는 방향으로 설계된다. 그전에 good proposal이 무엇인지 정의한다.

* good Proposal : $\Phi_{seg}^R$이 해당 영역에서 $\Phi_{seg}^I$보다 더 나은 예측을 돕게 하는 것

이러한 아이디어에 따라 RPN의 학습방식을 다음과 같이 수정한다.

* 각 proposal에 대해 해당 image region을 잘라서 $\Phi_{seg}^R$을 이용해 segmentation 예측 $P^R$을 수행

* 같은 proposal을 이용해 $\Phi_{seg}^I$의 예측 $P^I$ 또한 잘라냄

* 그 후 IoU 점수를 비교하여 판단
    * $P^I 보다 P^R$의 영역에서 IoU가 높다면 긍정적 proposal, 반대라면 부정적 proposal

이러한 방식으로 새롭게 label이 붙은 proposal을 기반으로 HDM은 더 나은 context를 가지는 영역을 제안하는 방법으로 학습된다.

위 그림처럼 HDM은 RDN과 달리 오토바이와 rider를 함께포함하는 영역을 제안하고 $\Phi_{seg}^R$은 rider로 인식할 수 있게 된다.

# 4. Experiments

## 4.1 Experimental setups

### 데이터셋

#### 1. NightCity+
- 원본 NightCity: 2998 train / 1299 val 이미지 존재
- pixel-wise annotation
- Cityscapes와 라벨 호환 (19 classes)
- validation set 일부 오라클 라벨 있어 재라벨링 → NightCity+로 명명
- 실험 결과는 모두 NightCity+ 기준
- 성능 향상 위해 Cityscapes도 보조 학습 데이터로 사용

#### 2. BDD100K-Night
- BDD100K에서 야간 이미지만 추출해 구성
- 총 343 train / 85 val 이미지
- class 수는 18개
- 학습 데이터 부족해 BDD100K 주간 이미지 7000장도 추가 사용

> Zurich-Dark, ApolloScape 등도 검토했으나 야간 학습에 부적합해서 제외

---

### 학습 설정

- 이미지 크기: 학습/추론 시 모두 1024×2048으로 리사이즈
- 입력 crop 크기: 512×1024
- augmentation:
  - random scale (0.5 ~ 2.0)
  - random flip
  - photometric distortion
  - normalization
- 평가 시 multi-scale augmentation 사용 (scales: 0.25, 0.5, 0.75, 1.0, 1.25)

---

### 학습 전략

- hard class 기준: mIoU < 0.5 클래스
- image-level ReLAM 학습 시:
  - Cityscapes (주간) / BDD100K-Night (야간) 데이터 split 사용
- region-level ReLAM 학습 시:
  - hard class crop 영역만 사용

---

### 구현 환경 및 세팅

- mmsegmentation 기반
- GPU: V100 8장
- batch size: GPU당 2개
- SyncBN 사용함
- baseline 결과도 직접 생성함 (NightCity+, BDD100K 기준)

---

### hyperparameter

- 모든 모델 80,000 iter로 학습
- segmentation 모듈은 UPer-Swin 설정 그대로 사용
- ReLAM:
- HDM: Faster R-CNN 기반 구조 따름
- 두 모듈 병렬 실행 가능 → Swin-Transformer와 같은 속도 유지

## 4.2 Experiment results

![Image5](/assets/images/Nightlab/image6.jpg){: .align-center}
### SOTA 비교

- NightLab-Baseline (이미지 수준만)도 기존 SoTA 모델인 UPer-Swin 보다 성능 높음
  - 공동 학습 시 UPer-Swin 대비 **+2.44%** 향상
- RDN 추가 → "NightLab-RDN"으로 명명됨
- NightLab-HDM (HDM 사용)은 성능 더 좋음
  - NightCity+: 60.73% → 62.82% (단일/조인트 기준) → **+2.48%, +3.15% 향상**
  - BDD100K-Night: 35.41%, 50.42% 기록 (단일/조인트 기준)

- DeeplabV3+ 기반 실험도 진행함  
  → NightLab-B + ReLAM + HDM 붙이면  
    - NightCity+: **+1.74% / +1.38% 향상**
    - BDD100K-Night: **+1.16% / +1.67% 향상**

---

### DA 방법과의 비교
![Image5](/assets/images/Nightlab/image7.jpg){: .align-center}

- UDA 방식은 대부분 unsupervised라서 직접 비교 어려움
- Pix2PixHD / CycleGAN 등 image translation 기반 방법 실험함
  - day→night 변환해서 supervised 방식으로 학습했지만,  
    → vanilla 모델보다 성능 낮음 (변형된 야간 이미지가 학습에 오히려 방해)
- SingleHDR (이미지 향상) 실험함 → 성능 변화 거의 없음

---

### UDA 사전학습 활용 여부

- DANNet [54], AdaptSeg [46] 사용해  
  - unlabeled 이미지로 pre-train 후,  
  - labeled 데이터로 finetune 해봄
- 성능 향상 없음  
  → 오히려 vanila training보다 미세하게 낮은 성능 보임
  → 도메인 적응된 특징(feature)이 편향되어 일반 supervised보다 열세

---

### 클래스별 성능 분석
![Image5](/assets/images/Nightlab/image8.jpg){: .align-center}

- NightLab은 대부분 클래스에서 성능 향상 보임
- 특히 pole, rider, motorcycle, bicycle에서 향상 큼
  - 예: pole 클래스 → 38.2% → 42.2%로 향상 (HDM + region-level 효과)
- terrain은 여전히 vegetation과 혼동되는 경향 존재

---

## 4.2.1 Ablation Study 요약

### NightLab-B 구조
![Image5](/assets/images/Nightlab/image9.jpg){: .align-center}
![Image5](/assets/images/Nightlab/image10.jpg){: .align-center}


- UPerNet의 decode head (UPerHead) 수정하여 사용
  - FPN + segmentation head
  - Deformable Conv 사용하면 성능 향상

- 실험 결과 (Table 5)
  - UPerNet-Swin baseline: 59.67%
  - NightLab-B: 60.37%
  - + RDN: 61.51%
  - + HDM: 62.01%
  - + ReLAM (image-level): +0.5% 향상
  - + ReLAM (region-level): 추가로 +0.3% 향상

→ 모듈 하나하나가 성능 향상에 기여한 것으로 확인됨

# 5. Conclusion

NightLab은 야간 image segmentation에 적합한 architecture로 context, light를 포함하는 두 단계 model을 사용하여 supervised 방법에서 SOTA를 달성했다.

그러나 여전히 주간 모델보다는 성능이 좋지 않기 때문에 추가적인 연구가 필요하다.