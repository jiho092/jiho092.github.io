---
layout: single
title:  "Segment Anything" #제목
excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - ["inpainting", "segmentation"] #테그

use_math: true

date: 2024-11-19
last_modified_at: 2024-11-19
#classes: wide    
---
[Segment Anything](https://arxiv.org/pdf/2304.02643){: .btn .btn--primary}

![Image5](/assets/images/SAM/image1.jpg){: .align-center}

&nbsp;&nbsp; 이 논문에서는 Image segmetation 분야에서 새로운 model, dataset, task를 제공하는 Segment Anything project를 진행한다. 여기서 사용되는 SAM(segment anything model)은 promptable하게 학습되어 zero-shot 전이 학습이 가능하며, 이 모델과 기존에 web-scale dataset이 존재하지 않아 어려웠기 때문에, SA-1B라는 새로운 데이터셋을 제공한다.

# 1.Introduction

web-scale dataset으로 pre-train된 LLM은 **Zero-shot** 및 **Few-shot** 을 통해 NLP 분야를 혁신하고 있다. 이 모델들은 train data와 다른 data의 분포에서도 일반화할 수 있다. 이것은  computer vision **(CLIP, ALIGN)** 에서도 적용하려 시도중이며, 특히 Image Segmentation 문제를 해결하기 위해 **SAM (Segment Anything Model)**이라는 모델이 제안되었다. SAM은 **prompt engineering**을 통해 새로운 데이터 분포에서도 일반화된 Segmentation 작업을 수행할 수 있습니다.

---

## **segmentation을 위한 세가지**
SAM의 개발은 다음 세 가지 핵심 질문에 기반을 두고 있다.

1. **task:** Zero-Shot 일반화를 가능하게 하는 promptable segmentation task는 무엇인가?  
2. **model:** 유연한 prompt와 빠른 Segmentation를 지원하는 model architecture는 무엇인가?  
3. **data:** 모델을 학습시키기 위한 대규모 dataset은 어디서 얻을 수 있는가?

---

## **Contribution**

### 1. **Promptable Segmentation Task**  
- prompt 기반의 segmentation task  
- 모호한(ambiguity) prompt에서도 합리적인 segmentation mask를 출력  
- 다양한 down stream에 활용 가능, zero-shot 일반화

---

### 2. **SAM (Segment Anything Model)**  
- **module architecture(세가지 제약조건)** 로 설계:  
  - **Image Encoder:** 입력 이미지를 embedding으로 변환.  
  - **prompt Encoder:** prompt(point, box, mask, text 등)를 embedding.  
  - **mask decoder:** 이미지와 prompt embedding을 결합하여 Segmentation mask 생성.  
- **interactive(interactive) 사용:**   
- **amoiguity 처리:** 하나의 prompt로 여러 mask를 생성하여 다양한 객체를 다루며 모호성 처리.

---

### 3. **Data Engine**  
- 일반화를 위한 SAM을 학습시키기 위해 대규모의 다양한 mask 데이터를 생성하는 방법  
- 기존 segmentation 데이터셋은 부족하므로, **Model-in-the-loop Data Generation** 을 활용
  1. **assisted manual:** 모델이 annotator를 보조하는 방식으로 데이터 생성.  
  2. **semi-automatic:** 모델이 일부 mask를 자동 생성하고, annotator는 남은 작업 수행.  
  3. **fully-automatic:** 모델이 모든 mask를 자동으로 생성.  
- 이를 통해 기존 segmentation 데이터셋보다 400배 많은 mask 데이터를 확보했다.  

---

### **SA-1B 데이터셋**
- **SA-1B 데이터셋:** 11M의 이미지와 10억 개 이상의 mask로 구성된 대규모 segementation 데이터셋.  
- **특징:**  
  - 고품질과 다양성을 유지하며 full-automatic된 방식으로 생성  
  - 기존 segmentation 데이터셋보다 400배 많은 mask 포함  
- **목표:**  
  - SAM 학습뿐만 아니라 향후 segmentation 연구에 활용될 수 있는 resource 제공

---

### **Responsible AI**
- 데이터셋 및 모델의 공정성과 편향 문제를 연구  
- SA-1B는 다양한 국가 및 경제적 background을 반영한 데이터를 포함하며, 다양한 집단 간 성능 차이가 적도록 설계  


---

### **Experiments**

- **23개의 segmentation 데이터셋**에서 SAM의 성능 평가.  
  - **Zero-shot Transfer:** 단일 point prompt로 높은 품질의 mask 생성.  
  - 기존 수동 annotation 데이터와 비슷한 수준의 정확도.  

- **다양한 down stream 작업에서 강력한 성능 :**  
  - edge detection, object proposal generation, instance segmentation 등.  
  - text-to-mask prediction  
- SAM은 train 데이터에 없는 새로운 object 및 image dstribution에도 적용 가능.

---


# **2. Segment Anything Task**

SAM은 NLP에서 "next token prediction"이 사전 학습과 다양한 down stream 작업을 해결하는 데 사용되는 것에 영감을 받았다. 이를 통해 segmentation에 적합한 task를 정의하고 SAM의 prompt 기반 segmentation 작업을 제안한다.

---

## **task**
- **Prompt:** 이미지에서 segmentation 대상을 나타냄  
  - ex) foreground/background 점, 박스 또는 mask, 자유 형식 텍스트 등  
- **목표:** 주어진 prompt에 대해 유효한 segmentation mask를 반환  
  - **valid mask:** prompt가 모호하더라도, 적어도 하나의 객체에 대해 합리적인 mask를 생성  
  - NLP의 언어 모델이 모호한 prompt에 대해 일관된 응답을 생성하는 것과 유사.  
- 이 작업은 자연스러운 사전 학습 알고리즘을 제공하며, zero-shot transfer를 가능하게 함

---

## **Pre-Training**
- **과정:**  
  1. 각 sample에 대해 다양한 prompt(점, 박스, mask 등)를 시뮬레이션
  2. 모델의 mask 예측 결과를 Ground Truth와 비교  
- **기존 interactive segmentation과 차이점:**  
  - interactive segmentation은 사용자의 입력을 통해 유효한 mask를 생성하는 것이 목표  
  - SAM은 **모든 prompt**에 대해 즉각적으로 유효한 mask를 생성하는 것이 목표
- 이 방식은 모호한 prompt를 포함하는 실제 사례에서 효과적인 모델을 학습하도록 보장

---

## **Zero-shot transfer**
- pre-training은 모델이 모든 prompt에 적절히 응답할 수 있는 능력을 제공  
- **prompt Engineering**을 통해 Down Stream 작업 해결 :  
  - ex) 고양이 bounding box detector를 이용해, 해당 박스를 prompt로 제공하여 고양이 instance segmentation 수행  
- 다양한 Segmentation 작업을 prompt로 변환 :  
  - automatic dataset labeling, Edge detection, object proposal generation 등

---

## **Related tasks & 차이점**
- **Segmentation :**  
  - interactive Segmentation, Edge detection, object proposal generation, foreground Segmentation, semantic Segmentation, instance Segmentation, panoptic Segmentation  
- **prompt 기반 Segmentation의 목표:**  
  - 광범위한 Segmentation 작업에 적응 가능한 모델 구축.  
  - **기존 multi-task 모델과 차이점:**  
    - multi-task 모델은 고정된 작업 세트를 처리하도록 설계.  
    - prompt 기반 Segmentation 모델은 학습 시 정의되지 않은 새로운 작업도 수행 가능.  

---

## **Discussion: prompt와 composition**
- **prompt와 composition의 장점:**  
  - 단일 모델을 확장 가능한 방식으로 활용  
  - 모델 설계 시점에 예측하지 못했던 작업도 수행  
- **ex)**  
  - CLIP은 DALL·E의 image-text alignment 구성 요소로 사용  
  - 유사하게, SAM은 더 큰 시스템의 Segmentation 구성 요소로 활용  
- **interactive Segmentation와 비교:**  
  - interactive Segmentation는 인간과의 상호작용을 위해 설계.  
  - prompt 기반 Segmentation 모델은 알고리즘 시스템에 통합 가능.  

---  

# **3. Segment Anything Model (SAM)**

Segment Anything Model (SAM)은 prompt 기반 Segmentation를 위해 설계된 모델로, **Image-Encoder**, **prompt-Encoder**, **mask-Decoder**로 이루어져 있다. 

![Image5](/assets/images/SAM/image2.jpg){: .align-center}

---

### 1. **Image-Encoder**
- 이미지에서 고해상도 embedding 생성
- **구조:**  
  - MAE로 사전 학습된 Vision Transformer (ViT) 사용.
  - 고해상도 input 처리를 위해 최소한으로 수정된 구조 채택.
- **작동 방식:**  
  - 이미지당 한 번 실행, prompt 적용 전에 계산 가능.

---

### 2. **Prompt-Encoder**
- 다양한 유형의 prompt(point, box, text, mask 등)를 embedding.
- **prompt 유형:**  
  1. **Sparse Prompts:** point, box, text.  
     - point, box: positional encoding과 학습된 prompt 타입 embedding의 합으로 표현.  
     - text: CLIP의 text encoder를 사용.  
  2. **Dense Prompts:** mask.  
     - convolution을 사용해 embedding을 생성하고, 이미지 embedding과 element-wise sum.

---

### 3. **Mask-Decoder**
- 이미지 embedding, prompt embedding, output token을 기반으로 mask 생성.
- **구조:**  
  - Transformer decoder block을 수정하여 사용.  
  - 동적 mask 예측 헤드를 통해 mask 생성.  
  - **bidirectional attetion mecansism:**  
    - prompt → image embedding.  
    - image embedding → prompt.  
  - output token을 동적 선형 분류기에 매핑하여 각 위치에서 mask foreground 확률 계산.
- **up-sampling:**  
  - 두 block 실행 후 이미지 embedding up-sampling 및 MLP를 통해 최종 mask 계산.

---

### **Resolving ambiguity**
- 모호한 prompt는 여러 유효한 mask를 생성할 수 있음.
- **해결 방법:**  
  - 하나의 prompt에 대해 여러 개의 출력 mask를 생성하도록 모델 설계.  
  - 보통 3개의 mask(전체, 부분, 하위 부분)로 충분.  
  - 학습 시, 최소 손실(minimum loss)을 역전파  
  - 모델은 각 mask에 대해 confidence score(ex: IoU)를 예측해 결정.

---

### **Efficiency**
- 사전 계산된 이미지 embedding을 기반으로, **prompt-Encoder**와 **mask-Decoder**가 작동.  
- **web browser에서 CPU로 실행 가능**  
- **실시간 interactive prompt**를 지원

---

### **Loss & training**
- **Loss:**  
  - Focal Loss와 Dice Loss의 선형 결합 사용  
- **training:**  
  - 다양한 geometric prompt를 혼합해 학습.  
  - interactive Segmentation를 시뮬레이션하여 11회 반복으로 prompt 샘플링.  
  - Data Engine(§4)과의 통합을 위해 설계.


#### Focal Loss와 Dice Loss

1. **Focal Loss**
Focal Loss는 **불균형 데이터 문제**를 해결하기 위해 제안된 loss다. 특히, 클래스 간 데이터 비율이 크게 차이 나는 **불균형 분류 문제**에서 효과적

* 정의
Focal Loss는 Cross Entropy Loss에 가중치를 부여하여 **쉽게 분류되는 샘플의 loss를 낮추고**, **어려운 샘플의 loss에 더 집중**하도록 설계

$$
FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

- $p_t$ : 모델이 정답 클래스에 할당한 확률
- $\alpha_t$ : class weight
- $\gamma$ : 조정 파라미터 (쉬운 샘플에 대한 loss 비중을 감소시킴)



2. **Dice Loss**
Dice Loss는 Segmentation에서 자주 사용되는 loss로, prediction 영역과 ground-truth 영역 의 **유사성**을 측정

* 정의
Dice Coefficient는 두 집합 간의 유사성을 나타내는 값으로 정의, Dice Loss는 이를 기반으로 함.


$$
Dice = \frac{2 |A \cap B|}{|A| + |B|}
$$

Dice Loss는 이를 loss로 변환한 형태로 다음과 같이 정의

$$
Dice\ Loss = 1 - \frac{2 \sum_i p_i g_i}{\sum_i p_i^2 + \sum_i g_i^2}
$$

- $p_i$: prediction
- $g_i$: Ground Truth

|   항목   | Focal Loss                                      | Dice Loss                                      |
|:--------:|------------------------------------------------|-----------------------------------------------|
|  주된 사용처 | 분류 문제 (특히 클래스 불균형)                  | 이미지 세그멘테이션                          |
|   목적    | 어려운 샘플에 집중                              | 예측된 마스크와 실제 마스크의 유사성 극대화  |
|  장점     | 불균형 데이터에서 성능 향상                     | 경계 및 소수 클래스 픽셀에서 성능 향상       |

---

# **4. Segment Anything Data Engine**

Segment Anything Data Engine은 **1.1B mask 데이터셋(SA-1B)** 를 구축하기 위해 설계된 데이터 생성 시스템이다. 데이터 엔진은 세 가지 주요 단계를 통해 mask를 수집한다:
1. **Assisted-manual Stage**
2. **Semi-automatic Stage**
3. **Full-automatic Stage**

---

## **1. Assisted-manual Stage**

### **개요**
- 전문 annotator가 SAM 기반 도구를 사용해 이미지 내 mask를 생성.
- Browser 기반 인터페이스에서 실시간으로 작동하며 interactive한 주석 제공.
- **주요 특징:**
  - foreground/background 객체를 클릭하거나, "브러쉬" 및 "지우개" 도구로 픽셀 정밀도 조정
  - semantic 제약 없음: "사물(things)"과 "재질(stuff)" 모두 자유롭게 주석 가능

### **진행 방식**
- **초기 데이터:** 공개 Segmentation 데이터셋을 기반으로 SAM을 사전 학습.  
- **반복:** 새로 주석된 mask만으로 SAM을 재학습하며 모델 성능 개선.  
- 모델 개선으로 평균 mask 주석 시간 감소: **34초 → 14초**.  
- 이미지당 mask 수 증가: **20 → 44**.  

### **결과**
- 12만 개 이미지에서 총 **430만 개 mask** 수집.  
- COCO 데이터셋의 mask 주석 대비 **6.5배 빠름**.

---

## **2. Semi-automatic Stage**

### **개요**
- mask의 다양성을 높이기 위해 자동 생성된 mask와 수동 주석을 병합.
- 덜 눈에 띄는 객체에 주목하도록 설계.

### **진행 방식**
1. **자동 검출:** 1단계에서 생성된 mask를 기반으로 객체를 탐지.       
2. **annotator:** 자동 생성된 mask가 포함된 이미지를 제공하고, 누락된 객체를 추가 주석.  
3. **반복:** 주기적으로 새로운 데이터를 추가하여 SAM을 5회 재학습.

### **결과**
- 추가로 **590만 개 mask**와 18만 개 이미지 확보.  
  - 총 **1,020만 개 mask**.
- auto mask 제외 시, 평균 주석 시간은 **34초**로 증가.  
  - 이미지당 mask 수: **44 → 72**(자동 mask 포함).

---

## **3. Fully Automatic Stage**

### **개요**
- 충분한 데이터 축적과 모델 개선을 통해 full-automatic된 mask 생성 단계로 전환.

### **핵심 기술**
1. **Ambiguity-aware Model:**  
   - 모델이 단일 점에서 유효한 여러 mask를 예측 가능.
   - 32×32 point prompt grid를 사용해 각 점에서 객체를 탐지.  
   - **모델 출력:** 하위 부분, 부분, 전체 객체 포함 mask 생성.  
2. **mask 검증:**  
   - 확률 맵 임계값 $(0.5 ± δ)$ 을 사용해 유사성을 확인하고 안정적인 mask만 선택.  
3. **후처리:**  
   - non-maximal suppression(NMS)를 통해 중복 제거.  
   - 작은 mask 품질 향상을 위해 이미지의 여러 겹치는 crop을 처리.

### **결과**
- 총 **1,100만 개 이미지**에서 **11억 개 mask** 생성.  

---

## **결론**
Segment Anything Data Engine은 Segmentation 데이터가 부족한 환경에서 SA-1B을 구축하며, SAM 모델의 성능 향상과 다목적 Segmentation 가능성을 강화했다. 이 것은 interactive 도구에서 Automatic system으로 발전하며, Segmentation 작업의 효율성을 극대화한다.


# **5. Segment Anything Dataset (SA-1B)**

SA-1B는 **11억 개의 고품질 Segmentation mask**와 **1,100만 개의 고해상도 이미지**로 구성된 대규모 데이터셋이다. 이 데이터는 Segment Anything Data Engine을 사용하여 생성되었으며, CV의 기초 모델 개발을 촉진하기 위해 설계되었다.  

---

## **Images**

- **총 이미지 수:** 1,100만 장  
- **해상도:** 평균 **3300×4950 픽셀**의 고해상도 이미지.  
  - 배포본은 짧은 변을 1500픽셀로 down-sampling하여 접근성과 저장 용량 문제를 완화.  
  - down-sampling 후에도 COCO 데이터셋의 약 3배 해상도.  

---

## **Masks**

- **총 mask 수:** 11억 개.  
  - **99.1%**는 full-automatic 방식으로 생성.  
- **Automatic mask 품질:**  
  - **500개 이미지(5만 개 mask)**를 샘플링해 전문가가 수정.  
  - 자동 mask와 전문가 수정 mask 간 평균 IoU(Intersection over Union):  
    - **90% 이상:** 94%.  
    - **75% 이상:** 97%.  
  - 기존 연구에서 annotator 간 일치도(IoU): **85~91%** [44, 60].  

---

## **Mask Properties**

### **Spatial Distribution of Object Centers**
- SA-1B는 LVIS v1 및 ADE20K 보다 **image corner**을 더 잘 커버.  
- COCO와 Open Images V5는 **center bias**이 더 뚜렷.  

### **데이터셋 크기 비교**
- SA-1B는 Open Images 대비:  
  - **11배 더 많은 이미지**.  
  - **400배 더 많은 mask**.  
  - **36배 더 많은 이미지당 mask 수**.  
- ADE20K와 비교해도 **3.5배 더 많은 이미지당 mask 수**.  

### **이미지 대비 mask 크기**
- SA-1B는 더 많은 mask를 포함하므로 **작거나 중간 크기 mask**의 비율이 더 큼.  

### **Shape Complexity**
- mask의 convex hull 대비 복잡도를 분석한 결과,  
  - 다른 데이터셋과 유사한 분포를 보임.  
  - mask 크기를 기준으로 stratified sampling하여 분석.  

---


# **6. Segment Anything RAI Analysis**

Segment Anything (SAM) 및 SA-1B에 대한 **Responsible AI** 분석은 데이터셋의 지리적·소득 기반 대표성과 SAM의 공정성을 중심으로 진행되었다. 이 과정에서 특정 속성(성별, 연령, 피부 톤) 간의 성능 차이를 평가한다.

---

## **1. Geographic and Income Representation**

- **SA-1B의 지역별 이미지 분포:**  
  - 상위 3개 국가는 서로 다른 대륙에 위치.  
  - **유럽**과 **아시아·오세아니아** 지역에서 COCO와 Open Images 대비 이미지 비율이 높음.  
  - **아프리카** 및 **저소득 국가**는 여전히 과소대표.  
    - 그러나 SA-1B는 모든 지역에 대해 최소 **2800만 개 mask**를 포함.  
    - 이는 기존 데이터셋의 전체 mask 수보다 10배 많음.  

- **mask 수 일관성:**  
  - 지역 및 소득 수준에 따라 이미지당 평균 mask 수가 일정 (94~108개).  

---

## **2. Fairness in Segmenting People**

### **분석 대상 속성**  
- **Gender Presentation**  
- **Age Group**  
- **Skin Tone**  

### **데이터 및 방법**  
- 성별 및 연령: **MIAP (More Inclusive Annotations for People)** 사용.  
- 피부 톤: 독점 데이터셋 사용. Fitzpatrick 피부 유형 (1=가장 밝음, 6=가장 어두움)으로 분류.  

#### **a. Gender**
- 기존 연구에 따르면 여성은 탐지 및 Segmentation 데이터셋에서 과소대표되는 경향  
- SAM은 그룹 간 유의미한 성능 차이를 보이지 않음.  

#### **b. Age**
- 기존 대규모 데이터셋에서 어린 연령대 및 고령층이 과소대표  
- SAM은 고령층으로 인식되는 그룹에서 성능이 가장 좋음   

#### **c. Skin**
- 대규모 데이터셋에서 밝은 피부 톤이 과대표되고 어두운 피부 톤이 과소대표되는 경향  
- SAM은 피부 톤 그룹 간 유의미한 성능 차이를 보이지 않음.  


---


# **7. Zero-Shot transfer**

 SAM이 학습되지 않은 작업과 데이터셋에서 얼마나 잘 일반화할 수 있는지를 평가하며, 이를 통해 SAM의 다양한 CV task에서의 유연성을 보여준다.
---

## **7.1 Zero-Shot Single Point Valid Mask Evaluation**

- **목표:** 단일 foreground point부터 객체를 분할.  
  - 하나의 point는 여러 객체를 지칭할 수 있어 모호할 가능성이 있음.  
  - 대부분의 데이터셋에서 모든 가능한 mask를 포함하지 않음.  

### **평가 지표**
- mIoU  
- **human study:** annotator가 mask 품질을 1점(무의미)에서 10점(픽셀 단위 완벽)으로 평가.


### **결과**
1. **mIoU:**
   - SAM은 23개 데이터셋 중 16개에서 RITM보다 높은 점수를 기록.
   - 모호성을 해결하는 oracle 방식에서는 SAM이 모든 데이터셋에서 RITM을 능가.

2. **human study:**
   - annotator는 SAM의 mask를 RITM보다 일관되게 높게 평가.
   - SAM의 평균 점수는 **7~9점**으로, 높은 식별성과 최소한의 오류를 나타냄.  


---

## **7.2 Zero-Shot Edge Detection**

- **목표:** SAM의 mask 예측 기능을 활용해 이미지에서 edge를 감지.
- BSDS500 (edge detection benchmark dataset).  

---

## **7.3 Zero-Shot Object Proposal**


## **7.4 Zero-Shot instance Segmentation**


## **7.5 Zero-Shot text-to-mask**
생략


## **7.6 분석 (Ablations)**

   - automatic mask만 사용해도 성능은 약간 감소(0.5 mIoU)하지만, 훈련 과정이 간소화

   - 11M 이미지에서 1M 이미지로 줄여도 강력한 성능 유지, SAM의 효율성을 입증.

   - ViT-H는 ViT-B 대비 큰 향상을 보였으나, ViT-L 대비 개선은 미미.

---

## **결론**
SAM은 Edge detection과 같은 저수준 작업부터 text-to-mask와 같은 고수준 작업에서 높은 Zero-Shot 성능을 보여준다. 