---
layout: single
title: "DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation"


excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-03-13
last_modified_at: 2025-03-13
classes: #wide    
---
[DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation](https://arxiv.org/pdf/2111.14887){: .btn .btn--primary}

# Abstract

&nbsp;&nbsp; 이미지의 semantic segmentation을 위해서 pixel 단위의 annotation을 수집하는 것은 비용이 많이 든다. 비용 문제를 해결하기 위해 systhetic data를 통해 모델을 학습한 후 real image로 adaptation하는 방법이 사용될 수 있다. 이것은 Unsupervised domain adaptation(UDA)라는 분야이다.

&nbsp;&nbsp; 기존의 많은 연구들이 새로운 adaptation 전략을 제안하고 있지만, 대부분 오래된 network architecture(CNN, DeepLab v2)를 사용하고 있다. 최근에 제안된 architecture의 영향을 체계적으로 연구한 적이 없기 때문에, 이 논문에서는 **UDA**를 위한 다양한 최신 Architecture를 벤치마킹하고, Transformer based network가 UDA segmentation에서 좋은 잠재력을 가진다는 것을 발견하였다.

&nbsp;&nbsp; 따라서 논문에서는 새로운 UDA 방법인 **DAformer**를 제안한다. 이 network는 Transformer Encoder와 Multi-level Context-aware Feature Fusion Decoder로 구성된다.

&nbsp;&nbsp; 또한 모델 학습을 안정화하고 source domain으로 부터 과적합 문제를 예방하기위해 세 가지 학습 전략을 사용하였다.

1. Rare Class Sampling

    - source domain에서 희귀한 class의 샘플을 증가시켜, self training의 corfirmation bias를 줄인다.
    - 이를 통해 pseudo-label의 품질을 향상 시킴

2. Thing-Class ImageNet Feature Distance

    - Thing Class(명확한 객체를 나타내는 clas : 차량, 사람)에 대한 특징을 ImageNet pre-trained model과 정렬하여 특징을 효과적으로 전달하도록 유도

3. Learning Rate Warmup

    - ImageNet pre-trained model의 특징 전달을 촉진하기 위해 학습 초기에 learning rate를 천천히 증가시키는 방법을 사용

&nbsp;&nbsp;  DAFormer는 UDA 분야에서 큰 성능 향상을 보였다. 특히, GTA $\to$ Cityscapes에서 기존 SOTA보다 10.8 mIoU가 향상되었고, Synthia $\to$ Cityscapes에서 5.4 mIoU 향상되었다. 또한 학습이 어려운 bus, truck과 같은 class에서도 높은 성능을 달성하였다.

# 1. Introduction

&nbsp;&nbsp; 딥러닝은 Computer vision에서 압도적인 성능을 보였지만, 많은 annotation data가 필요하다. 특히, segmentation에서는 image 하나의 annotation을 만드는데 1.5~3.3시간이 걸린다. 이를 해결하기 위한 한가지 방법은 Synthetic data를 사용하는 것인데, domain shift에 민감하고 생성 데이터에 결함이 있어 어려운 점이 있다. 이를 UDA 방법을 사용하여 해결한다.

&nbsp;&nbsp; 이전의 UDA 방법들은 DeepLabV2, FCN8s 등의 network architecture와 ResNet, VGG 등의 backborn을을 사용해왔다. 이 것들은 오래된 기법이기 때문에 최상의 성능을 내기 어렵다. 따라서 논문에서는 UDA가 미치는 영향을 정확히 분석하기 위해 진보된(Sophisticated) network를 사용하여 성능을 극대화 하는 방법을 연구했다. 그러나 최신 모델이 overfitting 등으로 인해 반드시 성능 향상을 이끄는 것은 아니기 때문에 UDA 환경에서 다양한 Semantic segmentation network를 선정하고 적절한 학습 전략을 선택한다. 이러한 연구를 통해 **DAFormer**를 제안한다.

&nbsp;&nbsp; 더 복잡하고 강력한 architecture는 adaptation 불안정성과 source domain에 대한 overfitting에 취약할 수 있다. 이를 위해 새로운 세 가지 학습 전략을 도입한다.

## 1. Rare Class Sampling(RCS)

- source domain에서 잘 등장하지 않는 rare class의 학습을 개선
- self training 과정에서 일반적인 class의 confirmation bias 완화
- rare class가 포함된 이미지를 더 자주 sampling하여 pseudo-label 품질 향상 및 안정화

## 2. Thing-Class ImageNet Feature Distance(FD)

- 다양한 표현을 가진 ImageNet의 특징을 통해 source domain의 학습을 정규화
- source domain의 데이터의 다양성 문제와 domain shift로 발생하는 문제를 해결
- FD 방법을 사용하지 않으면 source domain별 특징을 학습


## 3. Learning rate warmup

- 초기 학습에서 학습 속도를 의도된 값까지 선형적으로 증가시켜 학습 과정을 안정화
- ImageNet pre-training의 feature가 Semantic Segmentation으로 transfer 될 수 있도록 함

![Image5](/assets/images/Daformer/image1.jpg){: .align-center}

&nbsp;&nbsp; DAFormer는 network architecture와 전략이 UDA에서 중요한 역할을 한다는 연구들의 방법을 능가하고 있다.

## mIoU 증가 수준
* GTA $\to$ Cityscapes : 57.3 $\to$ 68.3
* Synthia $\to$ Cityscapes : 55.5 $\to$ 60.9

object 별 (mIoU)

* train : 16 $\to$ 65
* truck : 49 $\to$ 75
* bus : 59 $\to$ 78


# 2. Related work

## 2.1 Semantic Image Segmentation
생략

## 2.2 Unsupervised Domain Adaptation(UDA)

### 1. Adversarial Training based UDA

- Source Domain과 Target Domain의 분포를 Align

- Align 과정
    - input level
    - feature level
    - output level
    - patch level

### 2. Self-Training based UDA

- Target Domain에 대한 pseudo-label을 생성하여 학습
- 대부분의 UDA 방법이 pseudo-label을 사전 생성하여, 모델을 학습하고 반복


## 2.3 Data imbalanced solution

- 일반적인 data imbalance는 모델이 common classes에 편향되도록 한다.

- 해결 방법
    - Resampling : 특정 class가 많이 포함되도록 data 선택
    - Loss Re-weighting : 특정 class의 중요도를 높힘
    - Transfer Learning : pre-trained model을 활용하여 학습 성능 개선

- UDA에서도, Re-weihgting, Class-balanced Sampling 방법이 사용된다.(RCS)

# 3. Methods

## 3.1 Self-Training(ST) & UDA

&nbsp;&nbsp; 논문에서는 UDA의 기본적인 self-Training(ST)를 설명하고, 이를 통해 다양한 network architecture를 평가한다.

### UDA

* $g_{\theta}$ : nural network (source domain trained)
- source domain image : $ X_S = \{x_i^{(S)}\}_{i=1}^{N_S} $
- one-hot label : $ Y_S = \{y_i^{(S)}\}_{i=1}^{N_S} $
- target image : $ X_T = \{x_i^{(T)}\}_{i=1}^{N_T} $ (Target Labels 존재 X)

$$
L_S^{(\theta)} = - \sum_{j=1}^{H \times W} \sum_{c=1}^{C} y_S^{(j,c)} \log g_{\theta}(x_S^{(j,c)})
$$

&nbsp;&nbsp; 이 UDA 방법은 학습을 통해 Target Domain에서 좋은 성능을 보이는 것을 목표로 한다. 그러나 Target Domain의 label이 존재하지 않기 때문에 Source Domain에서의 Cross Entropy를 loss로 사용하여 학습을 하는데, 이로 인해 target domain에서 일반화되지 않아 성능이 저하되는 문제점을 가지고 있다.

### Self-Training

&nbsp;&nbsp; 위와 같은 문제를 해결하기 위해(Domain gap 줄이기) 여러 방법이 도입되었다. 크게 구분하면 Adversarial Training과 Self-Training이다. 이 중 논문에서는 Self Training 방법을 사용하되, Adversarial 방법과 결합하여 학습을 진행한다. 최근 연구에서는 Self Training이 더 좋은 기법으로 평가받고 있다. 

### Self Training에서의 Pseudo-Label 생성

&nbsp;&nbsp; Target Domain에서 특징을 더 잘 학습하기 위해 Self Training 방법에서는 Pseudo-Label(의사 라벨)을 생성한다. 이를 위해 Teacher Network $h_{\phi}$를 사용하여 target domain의 예측 확률 맵을 기반으로 label을 생성한다. 
$$
P_T^{(j,c)} = \left[ c = \arg\max_{c'} h_{\phi}(x_T)^{(j,c')} \right]
$$

- **[.] : Iverson bracket, 특정 조건을 만족하면 1, 그렇지 않으면 0**  
- **$\$h_{\phi}$ : Teacher Network**  
- **pseudo-label의 품질을 평가하기 위해 Confidence Threshold 기준 적용**  
- **모든 픽셀 중, 최대 softmax가 임계값을 초과하는 픽셀 수를 $q_T^{(\theta)}$로 정의**  


$$
q_T^{(\theta)} = \frac{\sum_{j=1}^{H \times W} [ \max_c h_{\phi}(x_T)^{(j,c)} > \tau ]}{H \times W}
$$

### Pseudo-label을 통한 Target Domain 학습

&nbsp;&nbsp; 앞선 방법으로 생성한 pseudo-label과 Confidence Threshold를 이용하여, Target Domain에서도 추가적인 학습을 할 수 있다. pseudo-label을 적용한 target daomin의 loss는 다음과 같다.

$$
L_T^{(\theta)} = - \sum_{j=1}^{H \times W} \sum_{c=1}^{C} q_T^{(\theta)} P_T^{(j,c)} \log g_{\theta}(x_T^{(j,c)})
$$

### Pseudo-label 생성 방식: Online vs Offline

* Online ST : 미리 pseudo-label을 생성한 후 학습
* offline ST : 훈련 중 실시간으로 pseudo-label을 생성하여 학습

&nbsp;&nbsp; 논문에서는 두가지 방법중 online STf를 사용하였다. 그 이유는 train 중 하나의 network만 사용하여 pseudo-label을 생성하기 때문에 연산 수가 감소하고, 다양한 network architecture를 비교하고 분석하기에 적합하기 때문에 online ST를 사용하였다.
&nbsp;&nbsp; Online ST에서는 teacher network $h_{\phi}$가 student network $g_{\theta}$의 weight 업데이트를 반영하며 학습되고, 보통 각 학습 단계 $t$에서 $h_{\phi}$의 network 가중치는 Exponential Moving Average(지수이동평균)으로 업데이트 된다. 학습이 진행될 수록 더 안정적인 예측을 한다.

$$
\phi_t^{+} \leftarrow \alpha \phi_t + (1 - \alpha) \theta_t
$$

* $\alpha$ : EMA의 smoothing factor

### Self Training의 장점
&nbsp;&nbsp; 앞선 연구들에 따르면 ST는 Augmented Target Data에서 $g_{\theta}$를 학습할 때 더 효과적이다. 이를 위해 $h_{\phi}$는 향상된 Augmentation이 적용된 비지도 학습과 결합되어 사용될 수 있다.
&nbsp;&nbsp; 논문에서는 DACS(Domian Adaptation via Cross-domain Consistency)기법을 통해 ST의 성능을 향상 시켰고, 컬러 변형, 가우시안 블러, ClassMix 등의 Augmentation 기법을 적용하여 Robustness를 강화하였다.

## 3.2 DAFormer Network Architecture

&nbsp;&nbsp; 기존 UDA 방법들은 단순화된 DeepLabV2를 많이 사용하였다. 그러나 DeepLabV2는 오래된 모델이기 때문에 논문에서는 UDA에 적합한 새로운 Network Architecture를 설계한다. 이것은 Supervised learning에서도 좋은 성능을 보이고, 우수한 Domain adaptation capabilities를 가진다. 전반적인 구조는 아래와 같다.

![Image5](/assets/images/Daformer/image2.jpg){: .align-center}

### 1. Encoder(MiT)

&nbsp;&nbsp; 논문에서는 Robust한 network architecture를 Encoder로 선택해야한다고 주장한다. Transformer의 Self Attention과 CNN convolution의 차이점을 살펴보자. 아래와 같은 이유로 Transformer가 Robust하다고 판단하여 Transformer architecture를 사용한다.

* 학습 방식
    1. CNN : 학습 과정에서 weight를 고정된 값으로 설정하고, test에서도 그대로 사용
    2. Self-Attention : input data token간의 Similarity를 사용하여 weight를 동적으로 계산

&nbsp;&nbsp; Transformer 모델 중 논문에서는 **Mix Transformers (MIT)**를 기반으로 Encoder를 설계하고 있다. MiT는 Semantic Segmentation에 최적화된 Transformer 구조로 이미지를 4 X 4 크기의 Small patch를 사용하여 분할한다. 따라서 ViT보다 세밀한 정보 유지를 할 수 있다.

### 2. Decoder (Context)

&nbsp;&nbsp; Transformer를 사용한 이전 Semantic Segmentation 연구에서는 Backborn의 output feature를 Decoder에서 Local 정보만을 활용하여 처리하는 방법을 사용해왔다. 그러나 DA Former에서는 **Context를 사용**하는 전략을 제안한다. 그 이유는 기존 연구들이 Bottleneck에서만 context를 고려하는 반면, DAFormer는 여러 Encoder level의 feature들을 사용하여 context 정보를 더욱 많이 반영하기 때문에 Context가 robustness를 증가시킨다.

* 구조

    * Encoder에서 출력된 feature map $F_i$를 동일한 채널 수 $C_e$로 변환
        * 1 X 1 Convolution 사용
        * Bilinear Upsampling을 통해 $F_i$의 feature size 확대후 concatenate
    * Context를 반영하는 **Context-aware Feature Fusion**
        * 여러 개의 3X3 Depthwise Separable Convolution 사용
        * 각 Convolution의 Dilation Rate(확장 비율)을 다르게 설정하여 Context 정보 확장
        * 최종적으로 1X1 Convolution을 사용하여 feature concat.
    * 기존 방법 ASPP(Atrous Spatial Pyramid Pooling)과의 차이점
        * 기존 방법은 bottle neck $F_4$에만 적용
        * DAFormer는 Multi-scale Features에 적용
        * Depthwise Seeparable Convolution을 통해 parameter 수가 줄어들어 source domain에 대한 overfitting 가능성을 줄임

## 3.3 Training Strategies for UDA

&nbsp;&nbsp; DAFormer에서는 source domain에 대한 overfitting을 방지하기 위해 UDA학습에서 stablize, regularize하는 세가지 방법을 제안한다.

1. **Rare Class Sampling(RCS)**
2. **Thing-Class ImageNet Feature Distance(FD)**
3. **Learning Rate Warmup**

### 1. Rare Class Sampling(RCS)

&nbsp;&nbsp; DAFormer는 기존 architecture들보다 어려운 class에서 좋은 성능을 보였지만, source domain에서 희귀한 class에 대해 UDA 성능이 불안정 했으며 큰 편차가 발생한다는 문제점이 있었다.

1. 문제점
    * random seed에 따라 rare class의 학습 시점이 달라지거나, 아예 이루어지지 않은 경우가 발생
    * 학습이 진행될수록 common class에 편향되는데 rare class가 나중에 학습되면 효과적인 학습이 불가능
    * Self-Training 과정에서 teacher network도 동일한 bias를 가지게 되어 문제가 더 심화됨

2. 해결책(RCS)

    * Source domain에서 각 class $c$의 픽셀 수를 기반으로 class frequency $f_c$ 계산

    $$
    f_c = \frac{\sum_{i=1}^{N_S} \sum_{j=1}^{H \times W} [y_S^{(j,c)}]}{N_S \cdot H \cdot W}
    $$

    * $f_c$를 기반으로 각 class $c$가 선택될 확률 $P(c)$를 결정

    $$
    P(c) = \frac{e^{(1 - f_c)/T}}{\sum_{c' = 1}^{C} e^{(1 - f_{c'})/T}}
    $$

&nbsp;&nbsp; $f_c$가 낮을수록 더 높은 확률로 Sampling되고, $T$는 분포의 smoothness를 조정한다. $T$가 낮을수록 rare class가 더 높게 sampling 된다.

&nbsp;&nbsp; **RCS** 방법을 통해서 bus, train, motocycle과 같은 희귀 class를 자주 sampling하여 성능을 안정적으로 만들고, 자연스럽게 공존하는 common class도 함께 학습하는 효과를 가진다.

### 2. Thing-Class Imagenet Feature Disntance(FD)

&nbsp;&nbsp; Semantic segmentation model $g_{\theta}$는 일반적으로 ImageNet Classfication modeㅣ로 pre-trained weight를 사용하여 초기화된다. 이 과정에서 Generic Feature가 활용되며, 모델이 빠르게 수렴하도록 도와준다. 그러나 ImageNet에는 real-world image가 포함되어, 일부 semantic segmentation class와 관련된 정보를 제공할 수 있다. 따라서 ImageNet pre-trained model이 segmentation에 supportive guidance를 제공할 가능성이 있다.

* ex) train, bus 등의 class는 UDA에서 학습이 어려움

&nbsp;&nbsp; 하지만, UDA 과정에서 DAFormer는 초기 학습에서는 일부 클래스들을 잘 구분하지만, 수백 번의 학습 단계가 지나면 잊어버리는 현상(forgetting)이 발생한다.(**4.5절**) 이는 결국 source domain의 loss $L_s$에 의해 주도되며, 모델이 synthetic source data에 과적합된다.

&nbsp;&nbsp; 이를 해결하기 위해 ImageNet에서 학습된 유용한 특징이 유지되도록 Feature Distance(FD) 기반의 정규화 기법을 사용한다.

* source domain image $x_s^{(i)}$의 bottleneck feature $F_{\theta}(x_S^{(i)})$와 ImageNet 모델의 feature $F_{ImageNet}(x_S^{(i)})$간의 L2 Distance 계산

$$
d^{(i,j)} = ||F_{ImageNet}(x_S^{(i)})_{(j)} - F_{\theta}(x_S^{(i)})_{(j)}||_2
$$

* FD Loss function은 Thing-class에 해당하는 픽셀에서만 계산

$$
L_{FD}^{(\theta)} = \frac{\sum_{j=1}^{H \times W} d^{(i,j)} \cdot M_{things}^{(i,j)}}{\sum_{j=1}^{H \times W} M_{things}^{(i,j)}}
$$

    * Thing-class : train, bus 등 (잘 정의되어 있는 class)
    * background class는 제외
    * $M_{things}$ : Thing-class를 나타내는 binary mask
$$
M_{things}^{(i,j)} = \sum_{c' = 1}^{C} y_{S,small}^{i,j,c'} \cdot [c' \in C_{things}]
$$

* 최종 Loss
$$
\mathcal{L} = \mathcal{L}_S + \mathcal{L}_T + \lambda_{FD} \mathcal{L}_{FD}.
$$

### 3. Learning Rate Warmup for UDA

&nbsp;&nbsp; 학습 초기에는 Learning rate를 Linearly warming up 방법으로 증가시키는 network가 일반화에 도움을 주는 것으로 알려져있다. CNN, Transformer 이 방법을 사용하고 있기 때문에 논문에서도 이 방법을 사용하고 있다.

$$
\eta_t = \eta_{base} \cdot \frac{t}{t_{warm}}
$$


# 4. Experiments

## 4.1 Implementation Details

### Datasets
- **Target Domain**: Cityscapes street scene dataset [13]  
  - 2,975 training images  
  - 500 validation images  
  - Resolution: 2048×1024  

- **Source Domain**:  
  - **GTA dataset** [59]: 24,966 synthetic images (1914×1052)  
  - **Synthia dataset** [61]: 9,400 synthetic images (1280×760)  

- **Common Practice in UDA** [72]:  
  - **Cityscapes**: Images resized to **1024×512**  
  - **GTA**: Images resized to **1280×720**  

### Network Architecture
- Implementation based on **MMSegmentation framework**
- **DAFormer architecture**:
  - Uses **MiT-B5 encoder** 
  - Feature pyramid with $C = [64, 128, 320, 512]$  
  - DAFormer decoder settings:
    - **Number of channels**: $C_e = 256$  
    - **Dilation rates**: $1, 6, 12, 18$  
  - **Encoders pretrained on ImageNet**  

### Training
- **Optimizer**: AdamW 
- **Learning rate**:  
  - Encoder: $\eta_{base} = 6 \times 10^{-5}$  
  - Decoder: $6 \times 10^{-4}$  
- **Weight decay**: 0.01  
- **Linear learning rate warmup**:  
  - $t_{warm} = 1.5k$ iterations  
  - Decays linearly after warmup  
- **Training iterations**: 40k  
- **Batch size**: 512×512 random crops  

### Data Augmentation
- **Following DACS** :
  - **Same data augmentation parameters**  
  - **Mixing ratio**: $\alpha = 0.99$ and $\tau = 0.968$  

### Sampling Strategies
- **RCS temperature**:  
  - $T = 0.01$  
  - Maximizes re-sampling of pixels with the least frequency  

### Feature Distance Regularization
- **Feature Distance (FD) weight**:  
  - $r = 0.75$  
  - $\lambda_{FD} = 0.005$  
  - Gradient magnitude similar to $L_S$  

## 4.2 Comparison of Network Architectures for UDA

&nbsp;&nbsp; 여기에서는 여러 Semantic Segmentation Network Architecture를 비교하고 UDA에서 얼마나 좋은 성능을 보이는지 비교한다.

**1️. Src-only (Source-only)**
- **정의**: Source domain 데이터만 사용하여 학습한 모델  
- **설명**:
  - Target domain의 데이터는 사용하지만 **레이블은 사용하지 않음**  


**2️. UDA (Unsupervised Domain Adaptation)**
- **정의**: Source과 Target 데이터를 함께 사용하여 UDA을 수행한 모델  
- **설명**:
  - Target Domain의 **레이블은 사용하지 않음** (비지도 학습)  
  - Self-training, Adversarial Training 등 다양한 UDA 기법을 사용하여  
    **Source 도메인에서 학습한 모델이 목표 도메인에서도 좋은 성능을 내도록 적응(adaptation)하는 과정 **  
  - Src-only 모델보다 **target 도메인에서 성능이 향상됨**  

**3. Oracle (Supervised Learning)**
- **정의**: Target domain의 **정확한 레이블을 사용하여 Supervised Learning 한 모델**  
- **설명**:
  - 목표 도메인의 데이터뿐만 아니라 **정확한 GT(Ground Truth) 라벨을 사용하여 학습**  
  - 따라서 **목표 도메인에서 최고의 성능을 보임**  
  - **하지만 현실적으로 목표 도메인의 GT 라벨을 구하는 것은 어려움**  
    → 연구에서 비교를 위한 상한선(Upper Bound)으로 사용됨  

![Image5](/assets/images/Daformer/image3.jpg){: .align-center}

* GTA → Cityscapes 변환에 대해 Table 1에서 성능을 비교함
* **domain generalization 성능**도 함께 제공
* supervised learning한 경우(Oracle 성능)도 포함
* mIoU(%)로 측정
* SegFormer가 UDA에서 좋은 성능을 보임

![Image5](/assets/images/Daformer/image4.jpg){: .align-center}
![Image5](/assets/images/Daformer/image5.jpg){: .align-center}

## 5. Conclusions

**DAFormer**는 Transformer Encoder와 context-aware fusion decoder를 기반으로 한 **UDA를 위한 네트워크 아키텍처**이다. 이를 통해, **Transformer가 UDA에서 가질 수 있는 잠재력**을 확인하였다. 또한, 우리는 **UDA를 안정화하고 정규화하기 위한 세 가지 학습 정책**을 도입하여 DAFormer의 성능을 더욱 향상시켰다.  

* SOTA(State-of-the-Art) 성능을 다음과 같이 향상시켰다.  
    - **GTA → Cityscapes**: **+10.8 mIoU 향상**  
    - **Synthia → Cityscapes**: **+5.4 mIoU 향상**  

DAFormer가 **DeepLabV2를 대체하여** 보다 높은 성능 수준에서 **UDA 방법들을 평가하는 중요한 기준이 될 것임을 강조한다.**
