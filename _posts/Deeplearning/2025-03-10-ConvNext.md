---
layout: single
title: "A ConvNet for the 2020s(ConvNeXt)
"

excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-03-10
last_modified_at: 2025-03-10
classes: #wide    
---
[A ConvNet for the 2020s(ConvNeXt)](https://arxiv.org/pdf/2201.03545){: .btn .btn--primary}


# Abstract

&nbsp;&nbsp; visual recognition의 광란의 20년대는 **VIT**의 도입으로부터 시작되었다. 또한 VIT는 ConvNets을 제치고 빠른 시일 내에 SOTA를 달성했다. 그러나 VIT는 object detection, semantic segmentation과 같은 일반적인 vision task 적용에서 어려움을 직면했고, **hierarchical Transformer(Swin Transformer 등)** ConvNet의 기술을 다시 도입하여 transformer를 일반적인 vision backnorn network로 사용할 수 있게 만들었으며, 다양한 vision task에서 좋은 성능을 보였다. 이러한 하이브리드 접근의 효과는 여전히 convolution의 고유한 bias 편향보다는 transformer의 고유한 우수성에 크게 기여한다. 이 논문에서는 design space를 재검토하고,  <span style='color:red'>**순수한 ConvNet이 달성할 수 있는 한계를 테스트한다.** </span> Vision Transformer 설계를 위해 standard ResNet을 점진적으로 현대화하고, 이 과정에서 성능에 영향을 주는 몇가지 주요 구성 요소를 확인한다. **이 연구의 결과는 ConvNeXt라고 부른다.** 

# 1. Introduction

&nbsp;&nbsp; 2010년대를 돌아보면 십수년동안 주목할만한 deep learning의 발전이 있었고, vision에서의 주된 driver는 CNN이었다. 특히 2010년대 AlexNet의 도입은 새로운 computer vision의 시대를 열었다. 이 분야는 빠른 속도로 발전했고, VGGNet, Inceptions, ResNet, DensNet 등의 ConvNets의 출현은 정확도, 효율성 및 확장성, 많은 유용한 디자인 원칙을 대중화하였다. 
&nbsp;&nbsp; 이중 computer vision에서 ConvNets 주요 전략은 **Sliding window strategy(CNN)**이고 특히 high-resolution에서 좋은 성능을 보였다. 동일한 시기에 자연어 처리분야에서는 CV와 다르게 Transformer가 RNN을 대체하며 지배적인 backborn network로 자리잡고 있었다. language와 vision domain사이의서의 불일치에도 불구하고, 두 stream은 2020년대 vision transformer의 도입으로 융합되었다. 이미지를 초기에 분할하는 patch layer를 제외하면 VIT는 이미지별로 귀납적 bias를 일으키지 않고, 원래의 NLP Transformer에서 작은 변화를 준다. VIT의 주된 초점 중 하나는 Scaling behavior에 있다. 거대한 모델의 크기와 데이터셋 덕분에 Transformer는 ResNet보다 더 좋은 성능을 보일 수 있었다. 이러한 결과들은 Classification에서 좋은 결과를 보였지만, Computer vision은 Classification에만 국한되지 않는다. 과거 다양한 CV task의 해결법은 fully-convolution paradigm, Sliding-window에 의존하였다. ConvNet의 귀납적 bias가 없다면, ViT model은 일반적인 vision backborn으로 채택되기에는 많은 문제가 발생한다. 가장 큰 문제는 VIT의 global attention design이다. 이는 input size에 대해 2차적인 복잡성을 가지고 있다. 이러한 문제들로 인해 ImageNet Classification정도는 적용이 가능하겠지만, 고해상도 이미지에 대해서는 다루기 어려울 것이다.
&nbsp;&nbsp; Hierarchical Transformers는 이러한 gap을 메꾸기 위해 하이브리드 접근법을 사용하였다(ex. sliding window 전략 재도입). Swin Transformer는 이 방향으로 나아가는 모델로, Transformer가 일반적인 vision backborn으로 채택되어 classification을 넘어서 다양한 vision task에서 SOTA를 달성할 수 있음을 보였다. <span style='color:red'>**Swin의 성공과 급진적인 적용은 또한 Convolution의 본질이 무의미해지는 것이 아니고, 여전히 발전할 방향이 많다는 것을 나타낸다.**</span> (backborn에서 개선할 부분이 남아있다.)
&nbsp;&nbsp; 이러한 양상속에서 Transformer의 많은 이점은 초점을 backborn convolution으로 바꾸었다. 그러나 이 시도는 cost가 많이 발생한다. 단순하게 sliding window self attention을 구현하는 것은 비쌀 수 있고, cyclic shifting과 같은 발전된 방법을 사용하는 것은 optimize 속도가 빠를 수는 있지만 시스템은 더욱 정교해진다. 반면, ConvNet이 이미 필요한 많은 속성을 충족한다는 것은 아이러니하다. ConvNet이 힘을 잃고 있는 유일한 이유는 Transformer가 많은 vision task에서 이를 능가하기 때문이며, 성능차이는 Transformer의 뛰어난 Scaling behavior에서 나타나고, multi head attention이 핵심 요소이다.
&nbsp;&nbsp; <span style='color:red'>**최근 문헌에서는 system level을 비교할 때 일반적으로 Swin과 ResNet이 선택된다.** (두 모델이 비슷한듯 다르며, 성능 차이가 발생)</span> ConvNets과 Swin은 similar inductive bias를 사용한다는 점이 유사하지만, training 과정이 다르고, macro/micro-level architecture design이 다르다. 논문에서는 이 두 모델에서 구분될만한 구조의 차이를 연구하고, network의 성능을 비교한다. 이 연구는 ConvNets의 VIT 이전과 이후 시대의 격차를 해소하고, ConvNet이 달성할 수 있는 한계를 테스트하기 위함이다. 
&nbsp;&nbsp; 이것을 수행하기 위해서 논문에서는 Standard ResNet을 향상된 방법으로 학습시켰고, 점진적으로 hierarchical vision Transformer의 구조를 현대화 시켰다. 연구에서의 키는 Transformer의 design이 ConvNet의 성능에 얼마나 영향을 미치는지이다. 이러한 초점에서 성능 차이에 기여하는 핵심 요소들을 발견하였다. 결과적으로 이것을 ConvNeXt라는 이름으로 제안하였고, Image Classification, Object Dectection, Semantic Segmentation 등 다양한 task에 적용시켰고, 놀랍게도 많은 benchmark에서 transformer와 경쟁할 수 있는 성능을 갖추었다.

# 2. Modernizing a ConvNet: a Roadmap
![Image5](/assets/images/Convnext/image1.jpg){: .align-center}
&nbsp;&nbsp; 이 챕터에서는 ResNet에서 ConvNet으로 이동하는 길을 알려주며, Transformer와 유사하다. FLOPs에 따른 두가지 모델 size를 고려한다. 논문에서는 간단하게 첫 번째 경우의 복잡성을 제시한다.

1. ResNet-50/Swin-T 체제 : $4.5 \times 10^9$ FLOPs (: 컴퓨팅 성능의 단위)
2. ResNet-200/Swin-B 체제 : $15.0 \times 10^9$ FLOPs

&nbsp;&nbsp; High-level에서 이 연구는 ConvNet으로서의 네트워크 단순성을 유지하면서 Swin과 다른 수준의 설계를 조사하고 따르는 것을 목표로 한다. 로드맵은 다음과 같다.

1. Starting point : ResNet-50
2. VIT와 유사한 방법으로 훈련시켜 기존 ResNet-50보다 향상된 성능을 얻음(baseline)
3. macro design, ResNeXt, inverted bottleneck, large kernel size, various layer-wise micro design에 대해 연구

&nbsp;&nbsp; 위 그림에서 과정의 결과가 나타나며, network modernization 각 스텝에서 결과를 도출했다. 네트워크 복잡도가 최종 성능과 관련이 있기 때문에, FLOPs는 대략적으로 제어되지만 중간에서는 FLOPs가 기준 모델과 다를 수 있다. 모든 실험은 ImageNet-1K로 진행되었다.

![Image5](/assets/images/Convnext/image2.jpg){: .align-center}
위 그림은 각 단계에 대한 성능표이다. 다음 과정들에 대해 알아보자.

## 2.1 Training Techniques

**ResNet-50(enhanced recipe)** (<span style='color:red'>**78.8%**</span>)

&nbsp;&nbsp; 최신 방법(VIT)와 동일한 학습 방법을 사용한다.

1. AdamW 사용
2. DeiT와 가까운 Hyper parameter를 사용
3. Data Augmentation (Mixup, Cutmix, RandAugment, Random Erasing ...)



## 2.2 Macro Design

&nbsp;&nbsp; Swin Transformers의 macro network에 대해 분석한다. Swin은 ConvNets과 같은 multi-stage design을 사용하며 각 단계마다 feature map의 크기가 다르다. 여기서 고려할 사항은 두가지가 있다.

1. Changing stage compute ratio **stage ratio**

- RestNet-50의 block 수를  (3,4,6,3)에서 (3,3,9,3)으로 바꾸어 Swin과 같은 비율로 만듦(<span style='color:red'>**79.4%**</span>)

2. Changing stem to "patchify" **'patchify' stem**

- ResNet에서는 $7 \times 7$ conv. layer와 stride : 2, max pooling을 사용하는데, Swin의 Patchify처럼 patch size와 비슷하게 $7 \times 7 \to 4 \times 4$로 변경하고 stride를 $2 \to 4$로 변경 (<span style='color:red'>**79.5%**</span>)


## 2.3 ResNeXt-ify
![Image5](/assets/images/Convnext/image5.jpg){: .align-center}
&nbsp;&nbsp; 여기서는 ResNeXt의 아이디어를 적용한다(기존 ResNet보다 FLOPs/Accuracy에서 더 좋은 성능을 보임). **ResNeXt의 핵심은 Group Convolution이다.(다른 Group과 convolution filter가 분리됨)** 
![Image5](/assets/images/Convnext/image3.jpg){: .align-center}

&nbsp;&nbsp; 논문의 연구에서는 depthwise convolution을 사용한다. 이 방법이 Self Attention에서 weighted sum과 비슷하기 때문이다. 또한depthwise convolution 후 $1 \times 1$ Conv.가 채널별 정보를 섞도록 도와준다. depthwise를 사용하면서 연산수가 줄었지만 정확도는 유지하였고, 연산량의 감소로 채널의 수를 증가(64 to 96)시켰다.
![Image5](/assets/images/Convnext/image4.jpg){: .align-center}
(<span style='color:red'>**80.5%**</span>)


## 2.4 inverted Bottleneck

&nbsp;&nbsp; Transformer에서 중요한 디자인은 Inverted bottleneck이다. 이것은 MLP black을 input size의 4배로 만드는 것인데, 이 구조를 이용한다. 이 경우 $1 \times 1$ shortcut conv. layer로 인해 채널 수가 감소하여 FLOPs은 줄어든다.
(<span style='color:red'>**80.6%**</span>)

## 2.5 Large Kernel Size

&nbsp;&nbsp; CNN에서는 일반적으로 $3 \times 3$ kernel을 사용하는데, GPU 성능이 발전하면서 Vision Transformer에서는 최소 $7 \times 7$를 사용하여 이를 적용해본다. 첫 단계로 depthwise conv.의 위치를 Transformer처럼 올리는 것인데, $3 \times 3$으로 FLOPs이 감소하지만 적용시 성능이 **79.9%**
로 감소한다. kernel size를 $7 \times 7$로 적용시 성능이 다시 상승한다.
(<span style='color:red'>**80.6%**</span>)

## 2.6 Micro Design
&nbsp;&nbsp; 이 섹션에서는 micro scale에서 차이를 확인한다. 대부분은 layer 수준에서 이루어진다.

![Image5](/assets/images/Convnext/image6.jpg){: .align-center}

* Fewer activation function
&nbsp;&nbsp; NLP와 vision에서 구조 차이중 한가지는 activation function이다. 여기에서는 activation function을 ReLU에서 GELU로 변경시킨다.(Swin이 GELU 사용)
&nbsp;&nbsp; 위 그림을 보면 Transformer에는 MLP에 activation이 하나만 존재하는 반면 ResNet에서는 normalization, activation이 각 layer마다 존재하여 이를 제거한다. 이 때부터 성능이 Swin-T와 비슷한 결과를 보인다.

(<span style='color:red'>**81.3%**</span>)

* Fewer normalization layers

&nbsp;&nbsp; Transformer는 MSA, MLP block 앞에서 normalization을 진행하여 이 구조를 ConvNext에도 적용시킨다.
(<span style='color:red'>**81.4%**</span>)

* Substituting BN with LN

&nbsp;&nbsp; Vision task에서는 Batch Normalization(BL)을 주로 사용해왔다. 그러나 NLP에서는 Layer Normalization(LN)을 많이 사용한다. 이를 적용하면 기존 ResNet에서는 성능이 약간 감소하지만, ConvNeXt에서는 성능이 향상되었다.

(<span style='color:red'>**81.5%**</span>)

* Separate downsampling layer

&nbsp;&nbsp; ResNet에서는 각 단게의 시작인 residual block에서 downsampling이 진행되는데 이때 $3 \times 3, stride : 2$를 사용한다. Swin에서는 layer들 사이에 근처 window를 합치며 사용하는데, 이 것과 비슷하게 $2 \times 2$ conv.를 사용한다.
(<span style='color:red'>**82.0%**</span>)

# 3. Empirical Evaluation on ImageNet

&nbsp;&nbsp; 앞선 연구들을 바탕으로 ResNet-50/200을 현대화 시켜 ConvNeXt-T/S/B/L/XL을 만들었다. 각 스테이지의 채널과 블락 수를 정리한 표는 다음과 같다.


![Image5](/assets/images/Convnext/image7.jpg){: .align-center}

또한 이전 모델들과의 구조 차이는 아래와 같다.
![Image5](/assets/images/Convnext/image8.jpg){: .align-center}

# 4. ConvNeXt 논문의 실험 세팅 정리

**ResNet과 Swin Transformer의 성능을 비교하며 ConvNeXt의 우수성을 입증**했다.  
다음은 논문에서 사용한 실험 셋팅이다.

---

## 1. 목적
1. **ResNet 대비 ConvNeXt 성능 비교**
2. **Swin Transformer 대비 ConvNeXt 성능 비교**
3. **Downstream task(Detection, Segmentation)에서 ConvNeXt의 성능 평가**

---

## 2. 모델 구성
###  ConvNeXt 주요 특징
- **Depthwise Convolution**
- **LayerNorm**
- **MLP 블록 활용 (ReLU → GELU)**
- **ResNet과 Swin Transformer의 장점 결합**

###  ConvNeXt 모델 구조
| 모델 | Depths | Channels |
|------|--------|---------|
| ConvNeXt-Tiny | [3, 3, 9, 3] | [96, 192, 384, 768] |
| ConvNeXt-Small | [3, 3, 27, 3] | [96, 192, 384, 768] |
| ConvNeXt-Base | [3, 3, 27, 3] | [128, 256, 512, 1024] |
| ConvNeXt-Large | [3, 3, 27, 3] | [192, 384, 768, 1536] |
| ConvNeXt-XL | [3, 3, 27, 3] | [256, 512, 1024, 2048] |

---

## 3. Classification (ImageNet-1K)
| 설정 항목 | 값 |
|-----------|-----------|
| **데이터셋** | ImageNet-1K |
| **Input Image Size** | 224x224 |
| **Optimizer** | AdamW |
| **Learning Rate (LR)** | 4e-3 (Cosine Decay) |
| **Batch Size** | 4096 |
| **Weight Decay** | 0.05 |
| **Epochs** | 300 |
| **Augmentation** | RandAugment, Mixup, CutMix |
| **Regularization** | Label Smoothing (0.1), DropPath (0.1 ~ 0.4) |

---

## 4. Object Detection (COCO)
- **Backbone:** ConvNeXt-Base, Large, XLarge  
- **Detection Framework:** Mask R-CNN, Cascade Mask R-CNN  
- **Training Schedule:** 1x (12 epochs) & 3x (36 epochs)  
- **Optimizer:** AdamW, LR = 1e-4  
- **Augmentation:** Multi-scale Training, Large Scale Jittering  

---

## 5. Semantic Segmentation (ADE20K)
- **Backbone:** ConvNeXt-Base, Large, XLarge  
- **Decoder:** UPerNet (Unified Perceptual Parsing Network)  
- **Input Image Size:** 512x512  
- **Optimizer:** AdamW, LR = 6e-5  
- **Batch Size:** 16  
- **Epochs:** 160K Iterations (~160 epochs)  
- **Augmentation:** Multi-scale Training, Random Crop  

---

# 6. 실험 결과 요약

## 1. ImageNet-1K 분류 성능
- ConvNeXt-Tiny **82.1% Top-1 Accuracy** (ResNet보다 +3% 향상)
- ConvNeXt-Large **85.8% Top-1 Accuracy** (Swin Transformer-L과 비슷한 성능)

## 2. Object Detection 성능 (COCO)
- ConvNeXt-Base + Mask R-CNN: **49.1 mAP**
- ConvNeXt-Large + Cascade Mask R-CNN: **53.2 mAP**  
- Swin Transformer보다 빠르고 성능이 비슷  

## 3. Semantic Segmentation 성능 (ADE20K)
- ConvNeXt-Base + UPerNet: **47.2 mIoU**  
- ConvNeXt-Large + UPerNet: **49.5 mIoU**  
- Swin-L과 동등한 성능  

---



# Conclusion

&nbsp;&nbsp; 2020년대에 VIT는 ConvNet을 밀어내며 자리를 차지하였다. 그러나 ConvNet이 부족한 것이 아니라 탐구가 덜 된 것이었다. 이 논문에서는 ConvNet을 현대화한 ConvNeXt를 제안하였으며, Swin보다 좋은 성능을 보이는 것을 확인할 수 있었다. 이로인해 CNN 모델은 여전히 경쟁력 있다는 것을 보여주고 있다.