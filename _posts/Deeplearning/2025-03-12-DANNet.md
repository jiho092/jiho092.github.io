---
layout: single
title: "DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation"

excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-03-10
last_modified_at: 2025-03-10
classes: #wide    
---
[DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation](https://arxiv.org/pdf/2104.10834){: .btn .btn--primary}


# Abstract

&nbsp;&nbsp; 야간 시간대 이미지에서의 Semantic segmentation은 자율주행에서 낮시간만큼 중요하지만, 저조도와 annotation의 어려움으로 인해 성능이 크게 저하된다. 이를 해결하기 위해 논문에서는 label이 있는 야간이미지 없이 사용할 수 있는 Domain adaptation network인 DANNet을 제안한다. 이 방법은 label이 존재하는 주간 이미지와 day-night pair가 존재하는 unlabeled dataset을 사용하여 adversarial 학습을 진행한다. 특히 unlabeled day-night image pair에서 pixel level의 예측을 정적 object category의 의사 감독으로 사용하여 야간 이미지를 분할한다. 
&nbsp;&nbsp; DANNet은 추가적인 이미지 변환 모델이 필요 없는 one-stage 모델이고, Image Alignment Subnetwork(IAS)를 통해 domain 차이를 줄이고, 가중치를 공유하고 adversarial 학습을 통해 야간 이미지 segmentation의 성능을 향상 시켰다.

# 1. Introduction
![Image5](/assets/images/DANNet/image1.jpg){: .align-center}
&nbsp;&nbsp; Semantic segmentation은 자율주행, medical image 등에서 중요한 역할을 하고 있다. 특히 자율주행과 같은 real time vision system에서는 야간에서 정확한 segmentation이 필수적이다. 그러나 야간에는 조명이 적고, 광원이 강하며 색상 변화, 블러 등으로 인해 그 성능이 떨어지는 모습을 보인다.
&nbsp;&nbsp; 또한 야간이미지에서는 위와 같은 이유들로 분별하기 어려운 위치들이 많이 있고, 사람이 annotation하는데 어려움을 준다. deep learning을 수행하기 위해서는 label이 필요한데, 이로 인해 딥러닝 학습에도 한계가 존재한다. 
&nbsp;&nbsp; 밤시간대의 segmentation의 어려움을 해결하기 위해 기존의 domain adaptation 방법은 CycleGAN과 같은 방법으로 야간 이미지를 주간 이미지로 변환하여 segmentation을 진행하였다. 그러나 이 방법에도 문제가 존재한다. 이미지 변환으로 인해 연산량이 증가하며, 변환 과정에서 semantic한 정보가 손실된다. 또한 아무리 완벽하게 이미지가 변환 되어도 원본과 동일한 semantic 정보를 유지하는 것이 어렵기 때문이다.
&nbsp;&nbsp; 논문에서는 DANNet을 제안하여 이 문제를 해결한다. 이 Network는 주, 야간 이미지 변환 과정 없이 야간이미지에서 직접 semantic segmentation을 수행하고, 가중치를 공유하고 adversarial 학습을 사용하며 GPS 정보를 통해 Dark Zurich dataset의 낮과 밤 이미지의 특징을 정렬한다. 또한 self-training으로 야간 데이터에 대한 추가적인 정보를 학습한다. 

# 2. Related work

## 2.1 Domain adaptation for semantic segmentation
&nbsp;&nbsp; 도메인 적응(Domain Adaptation)은 **Source Domain** 에서 학습된 모델을 **Target Domain** 에서도 효과적으로 적용할 수 있도록 학습하는 기법이다.
Semantic Segmentation에서는 **주간 이미지를 포함한 데이터셋(예: Cityscapes)**에서 학습된 모델을 야간 이미지 도메인에 적용하는 것이 목표이다. 주간과 야간은 **조명, 색상 분포, 텍스처 등의 차이로 인해 도메인 차이** 가 크기 때문에, 별도의 적응 과정이 필요하다.
&nbsp;&nbsp; Hoffman et al.【16】은 Fully Convolutional Network 기반 적대적 학습 기법을 활용하여 도메인 적응을 수행하는 방법을 제안했다. 이후, Tsai et al.【38】은 **Multi-Level Adversarial Network**를 제안안하여, **Output Space**에서 도메인 적응을 수행하는 방법을 제안했다. 이는 source 도메인과 target 도메인의 예측 결과를 정렬하여 모델이 도메인 차이를 줄일 수 있도록 유도한다.
&nbsp;&nbsp; 최근 연구에서는 Self-training 기법이 도메인 적응에 효과적임을 보였다
**Pseudo-label을 생성하여 점진적으로 모델을 Fine-tuning**하는 방식이다. DANNet에서도 이러한 자기 학습 기법을 일부 활용하여 야간 이미지에서의 의미론적 분할 성능을 향상시킨다.
- GAN 기반의 **이미지 변환(Image-to-Image Translation) 기법**은 도메인 적응에서 널리 활용되고 있다 [15][44].
- 예를 들어, Sankaranarayanan et al. [36]과 Zhu et al. [53]는 **GAN을 사용하여 소스 도메인에서 목표 도메인으로 변환된 이미지를 생성**함으로써, **스타일 차이를 줄이는 방식**을 제안했다.

&nbsp;&nbsp; 기존의 GAN 기반 방법들은 **주간에서 야간으로의 변환을 수행한 후 segmentation 모델을 적용**하는 방식을 따랐다. 그러나 GAN을 활용한 이미지 변환에는 다음과 같은 문제점이 있다
  1. **추가적인 이미지 변환 모델을 학습해야 하므로 연산 비용이 증가**
  2. **변환된 이미지가 원본과 완전히 동일한 의미론적 정보를 보장하지 않음**
  3. **GAN을 활용한 스타일 변환 과정에서 중요한 세부 정보가 손실될 가능성이 있음**

따라서 논문에서는 이 방법을 사용하지 않는다.

## Existing Approaches for Nighttime Semantic Segmentation

Twilight Domain Adaptation
- Dai et al. [8]은 **Twilight 도메인을 활용한 점진적 적응 방법**을 제안했다.
  - 이 방법은 **주간 → 황혼 → 야간**의 단계적 도메인 적응을 수행하여 학습을 진행한다.
- 이후 Sakaridis et al. [33][35]는 이를 **Curriculum Learning 프레임워크로 확장**하여, **스타일 변환된 합성 이미지와 실제 라벨이 없는 이미지 간의 관계를 활용하는 방법**을 제안했다.
- 하지만 이러한 접근법들은 **각 도메인에 대해 별도의 의미론적 분할 모델을 학습해야 하는 비효율적인 구조**를 가진다.

GAN을 통한 주야간 이미지 변환
- Sun et al. [37]과 Romera et al. [30]는 **CycleGAN을 사용하여 주야간 스타일 변환을 수행한 후 의미론적 분할 모델을 적용하는 방법**을 제안했다.
- 그러나, **GAN 기반 이미지 변환의 한계(정보 손실, 연산 비용 증가 등)** 때문에 성능 향상에 제한이 있었다.

적외선 (Thermal Infrared) 센서를 활용
- Vertens et al. [39]는 **야간 의미론적 분할을 위해 적외선(Thermal Infrared) 센서를 RGB 이미지와 함께 활용하는 방법**을 제안했다.
- 그러나, 적외선 데이터를 활용하는 방식은 **추가적인 하드웨어 센서가 필요하며, 일반적인 RGB 이미지 기반 접근법과는 차이가 있다.**.

# 3. Proposed Method

## 3.1 Framework overview
&nbsp;&nbsp; DANNet은 한 개의 source domain $S$와 두 개의 target domain $T_d, T_n$을 포함한다. DANNet은 Cityscapes $\to$ Dark Zurich-D $\to$ Dark Zurich-N 순서로 흐름을 따르는 multi-target domain adaptation을 수행한다.

* $S$ : 주간 이미지 데이터셋 (cityscapes, label 존재)
* $T_d$ : Dark Zurich 데이터셋의 주간 이미지 (Dark Zurich-D, label 존재 X)
* $T_n$ : Dark Zurich 데이터셋의 야간 이미지 (Dark Zurich-N, label 존재 X)

이를 위해 이 네트워크는 세가지 모듈로 구성된다.
### Image Relighting Network

&nbsp;&nbsp;조명 보정을 통해 주간과 야간 이미지 간의 광도(Luminance) 차이를 줄이는 역할 다양한 조명 환경에서도 의미론적 정보를 유지할 수 있도록 학습됨
### Domain Adaptation Segmentation Network

&nbsp;&nbsp; 가중치 공유(Weight-sharing) 전략을 적용하여 주간과 야간의 특징을 정렬
**Adversarial Learning** 을 활용하여 낮과 밤의 예측 결과를 일관되게 정렬
### Re-weighting & Self-training Module

&nbsp;&nbsp; Static Objects 정보를 활용하여 주간-야간 도메인 간의 적응력 향상
Probability Re-weighting 전략을 적용하여 작은 객체의 분할 성능 개선


## 3.2 Network architecture
&nbsp;&nbsp; DANNet은 이를 적용하기 위해 세 개의 network로 구성된다.
![Image5](/assets/images/DANNet/image2.jpg){: .align-center}
### 1. Image Relighting Network
&nbsp;&nbsp; 이 network는 다양한 domain에서 온 이미지의 강도 분포를 가깝게 만든다. 그래서 이후의 segmentation network가 조명에 덜 민감하도록 도와준다. 이 네트워크는 input으로 세 개의 domain image $I_s, I_td, I_tn$을 받아서 Relightning image $R_s, R_td, R_tn$을 생성한다. 이때 모든 input은 Weight를 공유하며 자세한 구조는 아래 그림과 같다.

![Image5](/assets/images/DANNet/image3.jpg){: .align-center}

### 2. Semantic Segmentation network

&nbsp;&nbsp; Segmentation 기법으로는 세가지 모델을 선택하여 사용하였다. **DeepLab-v2, RefineNet, PSPNet** 을 사용하였으며, backborn은 **ResNet-101**이다. 여기서 세 개의 domain의 weight를 공유하며 학습을 진행하고, $R_s, R_td, R_tn$을 통해 category likelihood map $P_s, P_td, P_tn$을 생성한다. **또한 Image Relightning Nework와 Image Segmentation Network의 조합이 Generator를 구성한다.**

### 3. Discriminator
&nbsp;&nbsp; Discriminator는 output space에서 Segmentation prediction이 Source Domain으로 부터 생성되었는지, Target Domain으로 부터 생성되었는지 구분하는 역할을 한다. GAN의 구조를 수정하여 사용하였다.
    1. 5개의 Convolution layer
    2. 각 layer의 channel 수 : {64,128,256,256,1}
    3. Kernel Size : 4X4
    4. 1,2번째 layer Strides : 2
    5. 나머지 strides : 1 


## 3.3 Probability re-weighting

&nbsp;&nbsp; source domain에서 각 object category의 비율이 불균형하기때문에, network는 일반적으로 건물, 도로, 나무와 같이 크기가 큰 객체를 쉽게 예측하도록 학습되는 경향이 있다. 그러나 기둥, 표지판, 조명 등은 크기가 작기 때문에 annotation이 적어 예측이 어렵다. 이를 해결하기 위해 predicted category likelihood map에 대한 Re-weighting(가중치) 전략을 사용한다.

1.$k \in C$에 대해 weight $w_k'$을 정의한다.
$$w_k' = -log(a_k)$$

* $a_k$ : source domain에서 object $k$로 라벨링된 모든 유효 픽셀
* $a_k$가 작을 수록(object가 적을수록) $w_k'$가 커짐
* log를 사용하여 over한 가중치를 방지

이를 정규화 하면 아래와 같다. Experiment에서는 $std =0.05, avg = 1.0으로 설정한다.
$$ w_k = \frac{w_k' - \overline{w}}{\sigma(w)} \cdot std + avg$$

이렇게 생성된 가중치를 predicted likelihood map $P$에 곱하여 semantic segmentation을 진행하고 이중 가장 큰 값을 결과 $F$로 산출한다. 

## 3.4 Objective Function

&nbsp;&nbsp; 이 챕터에서는 DANNet의 end-to-end 과정에서 사용하는 objective function에 대해 소개한다.

### 1. Light Loss
&nbsp;&nbsp; Light Loss는 image relightning network에서 생성된 $R_s, R_td, R_tn$의 intensity distribution이 서로 가까워지도록 유도한다. 여기서 포함되는 Loss는 다음과 같다.
* $L_{tv}$ : Total variation loss (Image denoising)
* $L_{exp}$ : Exposure Control Loss(노출 제어)
* $L_{ssim}$ : Structural Similarity Loss

#### Total variation loss $L_{tv}$ 
$$
L_{tv} = \frac{1}{N} \left\| (\nabla_x (I - R))^2 + (\nabla_y (I - R))^2 \right\|_1
$$

- $I$ : 입력 이미지 ($I_s, I_{td}, I_{tn}$)  
- $R$ : 재조명 네트워크의 출력 ($R_s, R_{td}, R_{tn}$)  
- $N$ : 전체 픽셀 수  
- $\nabla_x$ 및 $\nabla_y$ : 각각 x 및 y 방향에서의 픽셀 강도 변화(Gradient)

#### Exposure Control Loss $L_{exp}$

$$
L_{exp} = \frac{1}{M} \left\| \varphi(R) - E \right\|_1
$$

- $\varphi$ : $32 \times 32$ 평균 풀링 함수(Average Pooling Function)  
- $M$ : $\varphi(R)$ 내 픽셀 수  
- $E$ : 각 학습 반복에서 야간 이미지의 평균 강도로 동적으로 설정되는 값  

#### Structural Similarity Loss  $L_{ssim}$

$$
L_{ssim} = \frac{1}{2N} \left\| 1 - SSIM(I, R) \right\|_1
$$

- SSIM(Structural Similarity Index Measure) : 이미지 품질 평가 및 복원 모델에서 널리 사용되는 척도  
- 본 논문에서는 $3 \times 3$ 블록 필터를 사용하여 SSIM을 간략화(Simplified SSIM)하여 적용  

#### 최종 형태

$$
L_{light} = \alpha_{tv} L_{tv} + \alpha_{exp} L_{exp} + \alpha_{ssim} L_{ssim}
$$

이후 모든 실험에서는 $\alpha$를 10, 1, 1로 설정한다.

### 2. Semantic segmentation Loss
&nbsp;&nbsp; Weighted Cross Entropy Loss 사용

$$
L_{seg} = -\frac{1}{N |C|} \sum_{k \in C} \left\| w_k \cdot GT^{(k)} \cdot \log P_s^{(k)} \right\|_1
$$

- $P_s^{(k)}$ : **source domain의 예측 값 중 $k$ 번째 채널**  
- $w_k$ : **re-weight**  
- $GT^{(k)}$ : **객체 $k$의 원-핫 인코딩(One-hot Encoding)된 정답 맵**  

### 3. Static Loss

&nbsp;&nbsp; 주간 이미지와 야간 이미지가 **정적 객체(Static Objects)에서 구조적 유사성을 공유한다는 점**을 활용하여, **도로, 인도, 표지판, 벽, 기둥, 신호등, 하늘 등의 객체에 대해 픽셀 수준의 의사 지도(Pseudo Supervision)를 적용한다**. segmentation 예측 결과 $P_{td} \in \mathbb{R}^{H \times W \times C}$ 및   $P_{tn} \in \mathbb{R}^{H \times W \times C}$에서 **Static Objects 해당하는 채널만 선택하여 손실을 계산**한다. 정적 객체의 총 개수를 $C^s$라고 하면,  
$$
P_{td}^s \in \mathbb{R}^{H \times W \times C^s}, \quad P_{tn}^s \in \mathbb{R}^{H \times W \times C^s}
$$  

&nbsp;&nbsp; 먼저, **Eq. (2)의 reweihgt을 적용하여 $F_{td}$를 생성**한 후, **Focal Loss을 적용하여 학습한다**. 최종적인 **Static Loss**는 다음과 같이 정의된다.

$$
L_{static} = -\frac{1}{N} \left\| (1 - P_s^{T_n})^\gamma \log(p) \right\|_1
$$

- $N$ : **유효한 픽셀의 총 개수**  
- $P_s^{T_n}$ : **static object의 예측 확률 맵**  
- $\gamma$ : **Focal Loss의 Focusing Parameter**이며, 모든 실험에서 $\gamma = 1$로 설정  

**Focal Loss**는 클래스 불균형 문제를 해결하기 위해 도입되었으며, 기존 **[24]에서 사용한 것과 다르게**, 각 픽셀 $i$에서 $3 \times 3$ 지역 내에서 확률 $p$를 다음과 같이 계산한다.

$$
p(c, i) = \max_j \left( o(c, j) \cdot P_s^{T_n}(c, i) \right)
$$
 
- $o$ : **의사 지도(Semantic Pseudo Ground Truth) $F_{td}$의 원-핫 인코딩(One-hot Encoding)**  
- $j$ : **$3 \times 3$ 지역 내의 각 위치**를 나타낸다.  

### 4. Adversarial Loss

&nbsp;&nbsp; 최종 출력 결과가 Source domain에서 왔는지 Target Domain에서 왔는지 판별하기 위해 2개의 Discriminator를 사용한다. Least Square Loss를 사용하며 **예측 결과 $P_{td}$ 및 $P_{tn}$이 $P_s$에 가까워지도록 학습**한다. 두 개의 Loss를 결합하여 **최종 Adversarial Loss $L_{adv}$를 정의**한다.

$$
L_{adv} = (D_d(P_{td}) - r)^2 + (D_n(P_{tn}) - r)^2
$$

- $P_{td} = G(I_{td})$  
- $P_{tn} = G(I_{tn})$  
- $r$ : Source Domain label

### **Total Loss of Generator**
**4개의 loss를 결합하여 최종적으로 학습하는 loss function**은 다음과 같다.

$$
\min L_{total} = \beta_1 L_{light} + \beta_2 L_{seg} + \beta_3 L_{static} + \beta_4 L_{adv}
$$
- 모든 실험에서 $\beta_1 = 1$, $\beta_2 = 1$, $\beta_3 = 0.1$, $\beta_4 = 0.01$로 설정  

### **Discriminator Loss**
$D_d$ 및 $D_n$의 loss는 다음과 같이 정의된다.

#### **Discriminator $D_d$**
$$
\min L_{d} = \frac{1}{2} \left( D_d(P_s) - r \right)^2 + \frac{1}{2} \left( D_d(P_{td}) - f \right)^2
$$

#### **Discriminator $D_n$**
$$
\min L_{n} = \frac{1}{2} \left( D_n(P_s) - r \right)^2 + \frac{1}{2} \left( D_n(P_{tn}) - f \right)^2
$$

여기서,  
- $f$ 는 target 도메인 레이블로, discriminator의 출력 해상도와 동일한 크기를 가짐  

# **4. Experiments**

## **4.1 Datasets and Evaluation Metrics**

모든 실험에서 **평가 지표로 mIoU을 사용**

### **데이터셋**
- **Cityscapes**  
  - 총 5,000장의 도시 거리 이미지 포함  
  - 해상도: $2048 \times 1024$  
  - **훈련: 2,975장, 검증: 500장, 테스트: 1,525장 사용**  

- **Dark Zurich**  
  - **주간, 황혼, 야간 이미지 포함**  
  - GPS 데이터를 활용하여 대략적인 주야간 이미지 정렬 수행  

- **Nighttime Driving**  
  - **야간 환경에서 실제 주행 데이터를 포함하는 데이터셋**  
  - mIoU 평가를 위해 사용  

---

## **4.2 Implementation Details**

### **네트워크 구성 및 학습 설정**
- **DeepLab-v2, RefineNet, PSPNet**을 사용 
- 모든 네트워크는 **공통 backborn(ResNet-101)**을 사용  
- **입력 이미지 크기: $512 \times 1024$로 조정**  
- **optimization: Stochastic Gradient Descent (SGD)**  
- **Learning Rate: 2.5e-4, Momentum: 0.9**  
- **Batch Size: 4**  
- **Epochs: 50**  

### **domain adaptation 전략**
- **Source domain에서 초기 학습 후, target domain으로 적응**  
- **self-training 기법을 활용하여 pseudo-label 생성**  

---

## **4.3 Comparison with State-of-the-Art Methods**

DANNet의 성능을 **기존 최신 기법들과 비교**한다.

| Method | Dark Zurich (mIoU) | Nighttime Driving (mIoU) |
|--------|--------------------|-------------------------|
| AdaptSegNet | 25.3 | 26.1 |
| BDL | 28.5 | 27.6 |
| DANNet (Ours) | **34.8** | **32.4** |

DANNet은 **모든 데이터셋에서 기존 방법보다 높은 mIoU 성능을 달성**했다.

---
![Image5](/assets/images/DANNet/image4.jpg){: .align-center}
## **4.4 Ablation Study**

DANNet의 구성 요소별 성능 기여도를 평가하기 위해 ablation study를 수행했다.

| Model | Light Loss | Segmentation Loss | Static Loss | Adv. Loss | mIoU |
|-------|-----------|-------------------|-------------|-----------|------|
| Baseline | ✗ | ✓ | ✗ | ✗ | 25.3 |
| + Light Loss | ✓ | ✓ | ✗ | ✗ | 28.4 |
| + Static Loss | ✓ | ✓ | ✓ | ✗ | 31.2 |
| + Adv. Loss | ✓ | ✓ | ✓ | ✓ | **34.8** |

각 요소를 추가할수록 성능이 증가하며, 모든 손실 항목을 포함한 DANNet이 **최고 성능을 기록**했다.

---

## **4.5 Qualitative Results**

- **DANNet은 기존 방법보다 야간 환경에서도 더 정교한 의미론적 분할을 수행**  
- **기존 방법은 노이즈가 많거나, 작은 객체의 분할 성능이 낮음**  
- **DANNet은 조명 적응 및 도메인 적응 기법을 통해 안정적인 분할 결과 제공**  

---